{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "#%matplotlib nbagg\n",
    "from Object_detection_features import *\n",
    "import gym\n",
    "from gym import wrappers\n",
    "import lib\n",
    "from importlib import reload\n",
    "from skimage.transform import resize\n",
    "from skimage.color import rgb2gray\n",
    "\n",
    "% matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-01-23 21:22:03,172] Making new env: Skiing-v0\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('Skiing-v0')\n",
    "# Самая долгая часть. Считается один раз. В конструкторе находятся все классы объектов и фон\n",
    "odf = ObjectDetectionFeatures(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import model_from_json\n",
    "import json\n",
    "\n",
    "with open('./data/Encoder_21_01.txt', 'r') as model_file:\n",
    "     encoder = model_from_json(json.loads(next(model_file)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "encoder.load_weights('./data/Encoder_21_01.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-01-23 19:47:51,119] Making new env: Skiing-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n",
      "1000\n",
      "1500\n",
      "2000\n",
      "2500\n",
      "3000\n",
      "3500\n",
      "4000\n",
      "4500\n",
      "5000\n",
      "5500\n",
      "6000\n",
      "6500\n",
      "7000\n",
      "7500\n",
      "8000\n",
      "8500\n",
      "9000\n",
      "9500\n",
      "10000\n",
      "10500\n",
      "11000\n",
      "11500\n",
      "12000\n",
      "12500\n",
      "13000\n",
      "13500\n",
      "14000\n",
      "14500\n",
      "15000\n",
      "15500\n",
      "16000\n",
      "16500\n",
      "17000\n",
      "17500\n",
      "18000\n",
      "18500\n",
      "19000\n",
      "19500\n",
      "20000\n"
     ]
    }
   ],
   "source": [
    "max_observations = 20000\n",
    "sample_observations = []\n",
    "render = False\n",
    "count = 0\n",
    "\n",
    "env_name = 'Skiing-v0'\n",
    "env = gym.make(env_name)\n",
    "\n",
    "while True:\n",
    "    if len(sample_observations) >= max_observations: break\n",
    "    s = env.reset()\n",
    "    if count % 10 == 0:\n",
    "        observation = env.ale.getScreenGrayscale()\n",
    "        sample_observations.append(odf.get_simple_image(rgb2gray(observation[:, :, 0])))\n",
    "    count += 1\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        if render: env.render()\n",
    "        if len(sample_observations) >= max_observations: break\n",
    "        a = env.action_space.sample()\n",
    "        s, r, done, _ = env.step(a)\n",
    "        if count % 10 == 0:\n",
    "            observation = env.ale.getScreenGrayscale()\n",
    "            sample_observations.append(odf.get_simple_image(rgb2gray(observation[:, :, 0])))\n",
    "            if not len(sample_observations) % 500:\n",
    "                print(len(sample_observations))\n",
    "        a = env.action_space.sample()\n",
    "        count += 1\n",
    "        \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#sample_observations = np.array(sample_observations)\n",
    "#sample_observations = sample_observations.reshape(20000, 1, 60, 60)\n",
    "#sample_features = encoder.predict(sample_observations)\n",
    "#np.savez('./data/sample_features_20k.npz', sample_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sample_features = np.load('./data/sample_features_20k.npz')['sample_features']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000, 64)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mean = np.mean(sample_features, axis=0)\n",
    "std = np.std(sample_features, axis=0)\n",
    "state_transform = lambda state: (encoder.predict(odf.get_simple_image(rgb2gray((state) * 255).astype('uint8')).reshape([1, 1, 60, 60])).squeeze() - mean) / std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-01-23 21:24:22,851] Making new env: Skiing-v0\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('Skiing-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial replay filled with (100) records\n",
      "Initial replay filled with (200) records\n",
      "Initial replay filled with (300) records\n",
      "Initial replay filled with (400) records\n",
      "Initial replay filled with (500) records\n",
      "Initial replay filled with (600) records\n",
      "Initial replay filled with (700) records\n",
      "Initial replay filled with (800) records\n",
      "Initial replay filled with (900) records\n",
      "Initial replay filled with (1000) records\n",
      "Episode #1 ended! Reward: -28182\n",
      "Eps reduced to 0.7462\n",
      "Episodes: 1. Steps: 5000. Score: -28182.00 Alpha: 1e-05 Eps: 0.74625\n",
      "reward =  -5723.0\n",
      "q =  [-47.978 -47.989 -48.001]\n",
      "w norm =  [ 3.444  3.444  3.446]\n",
      "Episode #2 ended! Reward: -18725\n",
      "Eps reduced to 0.7425\n",
      "Episodes: 2. Steps: 10000. Score: -23453.50 Alpha: 1e-05 Eps: 0.74252\n",
      "reward =  -21030.0\n",
      "q =  [-98.955 -96.488 -96.41 ]\n",
      "w norm =  [ 7.098  6.92   6.915]\n",
      "Episode #3 ended! Reward: -36097\n",
      "Eps reduced to 0.7388\n",
      "Episodes: 3. Steps: 15000. Score: -27668.00 Alpha: 1e-05 Eps: 0.73881\n",
      "reward =  -19152.0\n",
      "q =  [-148.415 -144.592 -144.577]\n",
      "w norm =  [ 10.643  10.369  10.368]\n",
      "Episode #4 ended! Reward: -30548\n",
      "Eps reduced to 0.7351\n",
      "Episodes: 4. Steps: 20000. Score: -28388.00 Alpha: 1e-05 Eps: 0.73511\n",
      "reward =  -21590.0\n",
      "q =  [-224.642 -192.76  -192.719]\n",
      "w norm =  [ 16.109  13.823  13.82 ]\n",
      "Episode #5 ended! Reward: -37672\n",
      "Eps reduced to 0.7314\n",
      "Episodes: 5. Steps: 25000. Score: -30244.80 Alpha: 1e-05 Eps: 0.73144\n",
      "reward =  -18980.0\n",
      "q =  [-244.117 -240.908 -240.836]\n",
      "w norm =  [ 17.505  17.275  17.27 ]\n",
      "Episode #6 ended! Reward: -30000\n",
      "Eps reduced to 0.7278\n",
      "Episodes: 6. Steps: 30000. Score: -30204.00 Alpha: 1e-05 Eps: 0.72778\n",
      "reward =  -14085.0\n",
      "q =  [-288.901 -288.792 -288.814]\n",
      "w norm =  [ 20.716  20.708  20.71 ]\n",
      "Episode #7 ended! Reward: -30642\n",
      "Eps reduced to 0.7241\n",
      "Episodes: 7. Steps: 35000. Score: -30266.57 Alpha: 1e-05 Eps: 0.72414\n",
      "reward =  -15874.0\n",
      "q =  [-336.743 -336.371 -336.584]\n",
      "w norm =  [ 24.147  24.12   24.135]\n",
      "Episode #8 ended! Reward: -39198\n",
      "Eps reduced to 0.7205\n",
      "Episodes: 8. Steps: 40000. Score: -31383.00 Alpha: 1e-05 Eps: 0.72052\n",
      "reward =  -11780.0\n",
      "q =  [-384.501 -384.565 -384.542]\n",
      "w norm =  [ 27.571  27.576  27.574]\n",
      "Episode #9 ended! Reward: -19333\n",
      "Eps reduced to 0.7169\n",
      "Episode #10 ended! Reward: -25642\n",
      "Eps reduced to 0.7133\n",
      "Episode #11 ended! Reward: -12650\n",
      "Eps reduced to 0.7098\n",
      "Episodes: 11. Steps: 45000. Score: -28062.64 Alpha: 1e-05 Eps: 0.70977\n",
      "reward =  -3172.0\n",
      "q =  [-432.45  -433.056 -432.473]\n",
      "w norm =  [ 31.009  31.053  31.011]\n",
      "Episode #12 ended! Reward: -16539\n",
      "Eps reduced to 0.7062\n",
      "Episodes: 12. Steps: 50000. Score: -27102.33 Alpha: 1e-05 Eps: 0.70622\n",
      "reward =  -20165.0\n",
      "q =  [-482.365 -480.978 -487.92 ]\n",
      "w norm =  [ 34.588  34.489  34.987]\n",
      "Episode #13 ended! Reward: -30000\n",
      "Eps reduced to 0.7027\n",
      "Episodes: 13. Steps: 55000. Score: -27325.23 Alpha: 1e-05 Eps: 0.70269\n",
      "reward =  -15068.0\n",
      "q =  [-529.599 -529.565 -532.151]\n",
      "w norm =  [ 37.975  37.973  38.159]\n",
      "Episode #14 ended! Reward: -30000\n",
      "Eps reduced to 0.6992\n",
      "Episode #15 ended! Reward: -18303\n",
      "Eps reduced to 0.6957\n",
      "Episodes: 15. Steps: 60000. Score: -26902.07 Alpha: 1e-05 Eps: 0.69568\n",
      "reward =  -143.0\n",
      "q =  [-577.735 -577.723 -577.951]\n",
      "w norm =  [ 41.427  41.426  41.443]\n",
      "Episode #16 ended! Reward: -18906\n",
      "Eps reduced to 0.6922\n",
      "Episodes: 16. Steps: 65000. Score: -26402.31 Alpha: 1e-05 Eps: 0.6922\n",
      "reward =  -14223.0\n",
      "q =  [-624.027 -635.826 -623.967]\n",
      "w norm =  [ 44.747  45.592  44.743]\n",
      "Episode #17 ended! Reward: -39442\n",
      "Eps reduced to 0.6887\n",
      "Episodes: 17. Steps: 70000. Score: -27169.35 Alpha: 1e-05 Eps: 0.68874\n",
      "reward =  -9211.0\n",
      "q =  [-671.678 -671.651 -671.675]\n",
      "w norm =  [ 48.164  48.161  48.164]\n",
      "Episode #18 ended! Reward: -22881\n",
      "Eps reduced to 0.6853\n",
      "Episode #19 ended! Reward: -22095\n",
      "Eps reduced to 0.6819\n",
      "Episodes: 19. Steps: 75000. Score: -26676.58 Alpha: 1e-05 Eps: 0.68187\n",
      "reward =  -8360.0\n",
      "q =  [-718.731 -718.271 -718.127]\n",
      "w norm =  [ 51.538  51.504  51.495]\n",
      "Episode #20 ended! Reward: -30000\n",
      "Eps reduced to 0.6785\n",
      "Episodes: 20. Steps: 80000. Score: -26842.75 Alpha: 1e-05 Eps: 0.67846\n",
      "reward =  -3237.0\n",
      "q =  [-766.837 -766.473 -766.432]\n",
      "w norm =  [ 54.987  54.961  54.959]\n",
      "Episodes: 20. Steps: 85000. Score: -26842.75 Alpha: 1e-05 Eps: 0.67846\n",
      "reward =  -28174.0\n",
      "q =  [-812.818 -812.98  -813.517]\n",
      "w norm =  [ 58.284  58.296  58.335]\n",
      "Episode #21 ended! Reward: -30000\n",
      "Eps reduced to 0.6751\n",
      "Episode #22 ended! Reward: -24995\n",
      "Eps reduced to 0.6717\n",
      "Episodes: 22. Steps: 90000. Score: -26902.27 Alpha: 1e-05 Eps: 0.67169\n",
      "reward =  -7264.0\n",
      "q =  [-862.891 -860.859 -860.858]\n",
      "w norm =  [ 61.875  61.729  61.73 ]\n",
      "Episode #23 ended! Reward: -32824\n",
      "Eps reduced to 0.6683\n",
      "Episode #24 ended! Reward: -17870\n",
      "Eps reduced to 0.6650\n",
      "Episodes: 24. Steps: 95000. Score: -26772.67 Alpha: 1e-05 Eps: 0.66499\n",
      "reward =  -120.0\n",
      "q =  [-906.502 -906.589 -906.76 ]\n",
      "w norm =  [ 65.002  65.009  65.021]\n",
      "Episode #25 ended! Reward: -31065\n",
      "Eps reduced to 0.6617\n",
      "Episodes: 25. Steps: 100000. Score: -26944.36 Alpha: 1e-05 Eps: 0.66167\n",
      "reward =  -3956.0\n",
      "q =  [-951.893 -952.306 -952.015]\n",
      "w norm =  [ 68.257  68.287  68.266]\n",
      "Episode #26 ended! Reward: -32023\n",
      "Eps reduced to 0.6584\n",
      "Episodes: 26. Steps: 105000. Score: -27139.69 Alpha: 1e-05 Eps: 0.65836\n",
      "reward =  -5303.0\n",
      "q =  [ -996.517 -1011.982  -996.719]\n",
      "w norm =  [ 71.457  72.566  71.472]\n",
      "Episode #27 ended! Reward: -14769\n",
      "Eps reduced to 0.6551\n",
      "Episodes: 27. Steps: 110000. Score: -26681.52 Alpha: 1e-05 Eps: 0.65507\n",
      "reward =  -24435.0\n",
      "q =  [-1046.278 -1045.158 -1044.72 ]\n",
      "w norm =  [ 75.026  74.946  74.914]\n",
      "Episode #28 ended! Reward: -30000\n",
      "Eps reduced to 0.6518\n",
      "Episodes: 28. Steps: 115000. Score: -26800.04 Alpha: 1e-05 Eps: 0.65179\n",
      "reward =  -19330.0\n",
      "q =  [-1102.736 -1092.897 -1092.763]\n",
      "w norm =  [ 79.074  78.369  78.359]\n",
      "Episode #29 ended! Reward: -30000\n",
      "Eps reduced to 0.6485\n",
      "Episodes: 29. Steps: 120000. Score: -26910.38 Alpha: 1e-05 Eps: 0.64853\n",
      "reward =  -14239.0\n",
      "q =  [-1140.473 -1139.969 -1140.01 ]\n",
      "w norm =  [ 81.781  81.745  81.747]\n",
      "Episode #30 ended! Reward: -30000\n",
      "Eps reduced to 0.6453\n",
      "Episodes: 30. Steps: 125000. Score: -27013.37 Alpha: 1e-05 Eps: 0.64529\n",
      "reward =  -9122.0\n",
      "q =  [-1187.557 -1187.337 -1187.36 ]\n",
      "w norm =  [ 85.158  85.141  85.143]\n",
      "Episode #31 ended! Reward: -30000\n",
      "Eps reduced to 0.6421\n",
      "Episodes: 31. Steps: 130000. Score: -27109.71 Alpha: 1e-05 Eps: 0.64206\n",
      "reward =  -4210.0\n",
      "q =  [-1231.251 -1230.972 -1230.99 ]\n",
      "w norm =  [ 88.291  88.27   88.272]\n",
      "Episodes: 31. Steps: 135000. Score: -27109.71 Alpha: 1e-05 Eps: 0.64206\n",
      "reward =  -29264.0\n",
      "q =  [-1273.624 -1273.624 -1273.659]\n",
      "w norm =  [ 91.33   91.329  91.332]\n",
      "Episode #32 ended! Reward: -30000\n",
      "Eps reduced to 0.6389\n",
      "Episode #33 ended! Reward: -16922\n",
      "Eps reduced to 0.6357\n",
      "Episodes: 33. Steps: 140000. Score: -26888.58 Alpha: 1e-05 Eps: 0.63566\n",
      "reward =  -13818.0\n",
      "q =  [-1317.937 -1317.806 -1317.826]\n",
      "w norm =  [ 94.508  94.497  94.499]\n",
      "Episode #34 ended! Reward: -30000\n",
      "Eps reduced to 0.6325\n",
      "Episodes: 34. Steps: 145000. Score: -26980.09 Alpha: 1e-05 Eps: 0.63248\n",
      "reward =  -8818.0\n",
      "q =  [-1362.513 -1361.139 -1362.064]\n",
      "w norm =  [ 97.704  97.605  97.671]\n",
      "Episode #35 ended! Reward: -30000\n",
      "Eps reduced to 0.6293\n",
      "Episodes: 35. Steps: 150000. Score: -27066.37 Alpha: 1e-05 Eps: 0.62932\n",
      "reward =  -3719.0\n",
      "q =  [-1406.028 -1408.831 -1408.96 ]\n",
      "w norm =  [ 100.825  101.025  101.034]\n",
      "Episode #36 ended! Reward: -17912\n",
      "Eps reduced to 0.6262\n",
      "Episode #37 ended! Reward: -14086\n",
      "Eps reduced to 0.6230\n",
      "Episode #38 ended! Reward: -18489\n",
      "Eps reduced to 0.6199\n",
      "Episodes: 38. Steps: 155000. Score: -26258.16 Alpha: 1e-05 Eps: 0.61992\n",
      "reward =  -2736.0\n",
      "q =  [-1447.288 -1447.337 -1447.298]\n",
      "w norm =  [ 103.783  103.787  103.783]\n",
      "Episodes: 38. Steps: 160000. Score: -26258.16 Alpha: 1e-05 Eps: 0.61992\n",
      "reward =  -27665.0\n",
      "q =  [-1487.292 -1484.476 -1489.352]\n",
      "w norm =  [ 106.652  106.452  106.799]\n",
      "Episode #39 ended! Reward: -30000\n",
      "Eps reduced to 0.6168\n",
      "Episodes: 39. Steps: 165000. Score: -26354.10 Alpha: 1e-05 Eps: 0.61682\n",
      "reward =  -22636.0\n",
      "q =  [-1525.756 -1524.418 -1524.789]\n",
      "w norm =  [ 109.41   109.316  109.341]\n",
      "Episode #40 ended! Reward: -30000\n",
      "Eps reduced to 0.6137\n",
      "Episodes: 40. Steps: 170000. Score: -26445.25 Alpha: 1e-05 Eps: 0.61374\n",
      "reward =  -17584.0\n",
      "q =  [-1569.2   -1569.322 -1574.659]\n",
      "w norm =  [ 112.525  112.536  112.917]\n",
      "Episode #41 ended! Reward: -26036\n",
      "Eps reduced to 0.6107\n",
      "Episodes: 41. Steps: 175000. Score: -26435.27 Alpha: 1e-05 Eps: 0.61067\n",
      "reward =  -23964.0\n",
      "q =  [-1613.99  -1613.357 -1613.505]\n",
      "w norm =  [ 115.738  115.695  115.703]\n",
      "Episode #42 ended! Reward: -30000\n",
      "Eps reduced to 0.6076\n",
      "Episodes: 42. Steps: 180000. Score: -26520.14 Alpha: 1e-05 Eps: 0.60762\n",
      "reward =  -18905.0\n",
      "q =  [-1660.589 -1660.4   -1660.311]\n",
      "w norm =  [ 119.082  119.069  119.059]\n",
      "Episode #43 ended! Reward: -30000\n",
      "Eps reduced to 0.6046\n",
      "Episodes: 43. Steps: 185000. Score: -26601.07 Alpha: 1e-05 Eps: 0.60458\n",
      "reward =  -13690.0\n",
      "q =  [-1704.756 -1704.754 -1704.552]\n",
      "w norm =  [ 122.25   122.25   122.231]\n",
      "Episode #44 ended! Reward: -30000\n",
      "Eps reduced to 0.6016\n",
      "Episodes: 44. Steps: 190000. Score: -26678.32 Alpha: 1e-05 Eps: 0.60156\n",
      "reward =  -8540.0\n",
      "q =  [-1742.084 -1745.217 -1745.085]\n",
      "w norm =  [ 124.927  125.152  125.137]\n",
      "Episode #45 ended! Reward: -18847\n",
      "Eps reduced to 0.5985\n",
      "Episode #46 ended! Reward: -21986\n",
      "Eps reduced to 0.5956\n",
      "Episodes: 46. Steps: 195000. Score: -26406.07 Alpha: 1e-05 Eps: 0.59556\n",
      "reward =  -8219.0\n",
      "q =  [-1783.492 -1783.984 -1783.443]\n",
      "w norm =  [ 127.898  127.933  127.888]\n",
      "Episode #47 ended! Reward: -29409\n",
      "Eps reduced to 0.5926\n",
      "Episode #48 ended! Reward: -15649\n",
      "Eps reduced to 0.5896\n",
      "Episodes: 48. Steps: 200000. Score: -26244.52 Alpha: 1e-05 Eps: 0.58962\n",
      "reward =  -4049.0\n",
      "q =  [-1830.493 -1831.188 -1831.061]\n",
      "w norm =  [ 131.268  131.318  131.304]\n",
      "Episode #49 ended! Reward: -26111\n",
      "Eps reduced to 0.5867\n",
      "Episode #50 ended! Reward: -16123\n",
      "Eps reduced to 0.5837\n",
      "Episodes: 50. Steps: 205000. Score: -26039.42 Alpha: 1e-05 Eps: 0.58373\n",
      "reward =  -2824.0\n",
      "q =  [-1878.523 -1879.31  -1879.286]\n",
      "w norm =  [ 134.712  134.771  134.764]\n",
      "Episode #51 ended! Reward: -15243\n",
      "Eps reduced to 0.5808\n",
      "Episode #52 ended! Reward: -20456\n",
      "Eps reduced to 0.5779\n",
      "Episode #53 ended! Reward: -17120\n",
      "Eps reduced to 0.5750\n",
      "Episodes: 53. Steps: 210000. Score: -25562.08 Alpha: 1e-05 Eps: 0.57502\n",
      "reward =  -666.0\n",
      "q =  [-1927.367 -1927.887 -1933.702]\n",
      "w norm =  [ 138.215  138.259  138.669]\n",
      "Episode #54 ended! Reward: -16077\n",
      "Eps reduced to 0.5721\n",
      "Episodes: 54. Steps: 215000. Score: -25386.43 Alpha: 1e-05 Eps: 0.57215\n",
      "reward =  -18027.0\n",
      "q =  [-1978.015 -1986.396 -1982.553]\n",
      "w norm =  [ 141.849  142.458  142.174]\n",
      "Episode #55 ended! Reward: -32893\n",
      "Eps reduced to 0.5693\n",
      "Episodes: 55. Steps: 220000. Score: -25522.91 Alpha: 1e-05 Eps: 0.56929\n",
      "reward =  -18979.0\n",
      "q =  [-2030.529 -2027.53  -2027.75 ]\n",
      "w norm =  [ 145.615  145.411  145.416]\n",
      "Episode #56 ended! Reward: -35628\n",
      "Eps reduced to 0.5664\n",
      "Episodes: 56. Steps: 225000. Score: -25703.36 Alpha: 1e-05 Eps: 0.56644\n",
      "reward =  -17285.0\n",
      "q =  [-2082.588 -2079.377 -2080.308]\n",
      "w norm =  [ 149.349  149.129  149.187]\n",
      "Episode #57 ended! Reward: -30000\n",
      "Eps reduced to 0.5636\n",
      "Episodes: 57. Steps: 230000. Score: -25778.74 Alpha: 1e-05 Eps: 0.56361\n",
      "reward =  -12098.0\n",
      "q =  [-2127.22  -2126.842 -2126.556]\n",
      "w norm =  [ 152.551  152.535  152.503]\n",
      "Episode #58 ended! Reward: -30000\n",
      "Eps reduced to 0.5608\n",
      "Episodes: 58. Steps: 235000. Score: -25851.52 Alpha: 1e-05 Eps: 0.56079\n",
      "reward =  -6947.0\n",
      "q =  [-2173.651 -2172.631 -2172.586]\n",
      "w norm =  [ 155.882  155.818  155.803]\n",
      "Episode #59 ended! Reward: -30000\n",
      "Eps reduced to 0.5580\n",
      "Episodes: 59. Steps: 240000. Score: -25921.83 Alpha: 1e-05 Eps: 0.55799\n",
      "reward =  -2020.0\n",
      "q =  [-2216.201 -2214.401 -2214.463]\n",
      "w norm =  [ 158.935  158.813  158.806]\n",
      "Episodes: 59. Steps: 245000. Score: -25921.83 Alpha: 1e-05 Eps: 0.55799\n",
      "reward =  -27341.0\n",
      "q =  [-2252.987 -2252.269 -2252.475]\n",
      "w norm =  [ 161.576  161.528  161.532]\n",
      "Episode #60 ended! Reward: -30000\n",
      "Eps reduced to 0.5552\n",
      "Episodes: 60. Steps: 250000. Score: -25989.80 Alpha: 1e-05 Eps: 0.5552\n",
      "reward =  -22488.0\n",
      "q =  [-2289.413 -2288.2   -2288.93 ]\n",
      "w norm =  [ 164.189  164.104  164.147]\n",
      "Episode #61 ended! Reward: -30000\n",
      "Eps reduced to 0.5524\n",
      "Episodes: 61. Steps: 255000. Score: -26055.54 Alpha: 1e-05 Eps: 0.55242\n",
      "reward =  -17376.0\n",
      "q =  [-2332.486 -2332.538 -2331.967]\n",
      "w norm =  [ 167.278  167.284  167.234]\n",
      "Episode #62 ended! Reward: -30000\n",
      "Eps reduced to 0.5497\n",
      "Episodes: 62. Steps: 260000. Score: -26119.16 Alpha: 1e-05 Eps: 0.54966\n",
      "reward =  -12178.0\n",
      "q =  [-2363.769 -2364.691 -2364.046]\n",
      "w norm =  [ 169.522  169.591  169.537]\n",
      "Episode #63 ended! Reward: -21200\n",
      "Eps reduced to 0.5469\n",
      "Episodes: 63. Steps: 265000. Score: -26041.08 Alpha: 1e-05 Eps: 0.54691\n",
      "reward =  -23512.0\n",
      "q =  [-2395.768 -2397.941 -2396.577]\n",
      "w norm =  [ 171.816  171.976  171.872]\n",
      "Episode #64 ended! Reward: -33420\n",
      "Eps reduced to 0.5442\n",
      "Episode #65 ended! Reward: -17525\n",
      "Eps reduced to 0.5415\n",
      "Episode #66 ended! Reward: -21481\n",
      "Eps reduced to 0.5387\n",
      "Episodes: 66. Steps: 270000. Score: -25954.76 Alpha: 1e-05 Eps: 0.53875\n",
      "reward =  -1812.0\n",
      "q =  [-2442.565 -2443.235 -2453.751]\n",
      "w norm =  [ 175.172  175.224  175.977]\n",
      "Episode #67 ended! Reward: -15102\n",
      "Eps reduced to 0.5361\n",
      "Episode #68 ended! Reward: -16670\n",
      "Eps reduced to 0.5334\n",
      "Episodes: 68. Steps: 275000. Score: -25658.62 Alpha: 1e-05 Eps: 0.53337\n",
      "reward =  -9760.0\n",
      "q =  [-2491.214 -2490.952 -2491.892]\n",
      "w norm =  [ 178.66   178.649  178.714]\n",
      "Episode #69 ended! Reward: -30000\n",
      "Eps reduced to 0.5307\n",
      "Episodes: 69. Steps: 280000. Score: -25721.54 Alpha: 1e-05 Eps: 0.53071\n",
      "reward =  -4714.0\n",
      "q =  [-2538.396 -2538.614 -2538.287]\n",
      "w norm =  [ 182.047  182.069  182.044]\n",
      "Episodes: 69. Steps: 285000. Score: -25721.54 Alpha: 1e-05 Eps: 0.53071\n",
      "reward =  -29672.0\n",
      "q =  [-2584.724 -2585.967 -2584.85 ]\n",
      "w norm =  [ 185.373  185.465  185.384]\n",
      "Episode #70 ended! Reward: -30000\n",
      "Eps reduced to 0.5281\n",
      "Episodes: 70. Steps: 290000. Score: -25782.66 Alpha: 1e-05 Eps: 0.52805\n",
      "reward =  -24663.0\n",
      "q =  [-2631.582 -2627.015 -2627.119]\n",
      "w norm =  [ 188.735  188.41   188.418]\n",
      "Episode #71 ended! Reward: -37754\n",
      "Eps reduced to 0.5254\n",
      "Episode #72 ended! Reward: -29668\n",
      "Eps reduced to 0.5228\n",
      "Episodes: 72. Steps: 295000. Score: -26002.89 Alpha: 1e-05 Eps: 0.52278\n",
      "reward =  -672.0\n",
      "q =  [-2668.185 -2669.074 -2668.542]\n",
      "w norm =  [ 191.361  191.427  191.387]\n",
      "Episode #73 ended! Reward: -25917\n",
      "Eps reduced to 0.5202\n",
      "Episodes: 73. Steps: 300000. Score: -26001.71 Alpha: 1e-05 Eps: 0.52017\n",
      "reward =  -8786.0\n",
      "q =  [-2714.811 -2714.826 -2714.948]\n",
      "w norm =  [ 194.705  194.707  194.718]\n",
      "Episode #74 ended! Reward: -17827\n",
      "Eps reduced to 0.5176\n",
      "Episode #75 ended! Reward: -18957\n",
      "Eps reduced to 0.5150\n",
      "Episodes: 75. Steps: 305000. Score: -25798.79 Alpha: 1e-05 Eps: 0.51498\n",
      "reward =  -13973.0\n",
      "q =  [-2762.855 -2763.444 -2762.568]\n",
      "w norm =  [ 198.151  198.199  198.138]\n",
      "Episode #76 ended! Reward: -30000\n",
      "Eps reduced to 0.5124\n",
      "Episode #77 ended! Reward: -16443\n",
      "Eps reduced to 0.5098\n",
      "Episodes: 77. Steps: 310000. Score: -25731.84 Alpha: 1e-05 Eps: 0.50985\n",
      "reward =  -1489.0\n",
      "q =  [-2809.363 -2810.11  -2810.286]\n",
      "w norm =  [ 201.486  201.553  201.56 ]\n",
      "Episodes: 77. Steps: 315000. Score: -25731.84 Alpha: 1e-05 Eps: 0.50985\n",
      "reward =  -26590.0\n",
      "q =  [-2858.773 -2842.921 -2842.581]\n",
      "w norm =  [ 205.031  203.91   203.877]\n",
      "Episode #78 ended! Reward: -30000\n",
      "Eps reduced to 0.5073\n",
      "Episodes: 78. Steps: 320000. Score: -25786.56 Alpha: 1e-05 Eps: 0.5073\n",
      "reward =  -21511.0\n",
      "q =  [-2880.072 -2879.426 -2879.841]\n",
      "w norm =  [ 206.561  206.53   206.552]\n",
      "Episode #79 ended! Reward: -30000\n",
      "Eps reduced to 0.5048\n",
      "Episodes: 79. Steps: 325000. Score: -25839.90 Alpha: 1e-05 Eps: 0.50476\n",
      "reward =  -16489.0\n",
      "q =  [-2924.427 -2920.101 -2919.786]\n",
      "w norm =  [ 209.748  209.446  209.416]\n",
      "Episode #80 ended! Reward: -30000\n",
      "Eps reduced to 0.5022\n",
      "Episodes: 80. Steps: 330000. Score: -25891.90 Alpha: 1e-05 Eps: 0.50224\n",
      "reward =  -11427.0\n",
      "q =  [-2962.243 -2962.227 -2962.19 ]\n",
      "w norm =  [ 212.468  212.468  212.459]\n",
      "Episode #81 ended! Reward: -34649\n",
      "Eps reduced to 0.4997\n",
      "Episodes: 81. Steps: 335000. Score: -26000.01 Alpha: 1e-05 Eps: 0.49972\n",
      "reward =  -11307.0\n",
      "q =  [-2993.372 -2988.205 -2984.431]\n",
      "w norm =  [ 214.701  214.331  214.057]\n",
      "Episode #82 ended! Reward: -30000\n",
      "Eps reduced to 0.4972\n",
      "Episodes: 82. Steps: 340000. Score: -26048.79 Alpha: 1e-05 Eps: 0.49723\n",
      "reward =  -6299.0\n",
      "q =  [-3014.516 -3019.396 -3016.143]\n",
      "w norm =  [ 216.222  216.57   216.334]\n",
      "Episode #83 ended! Reward: -32879\n",
      "Eps reduced to 0.4947\n",
      "Episodes: 83. Steps: 345000. Score: -26131.08 Alpha: 1e-05 Eps: 0.49474\n",
      "reward =  -7359.0\n",
      "q =  [-3065.2   -3056.396 -3057.143]\n",
      "w norm =  [ 219.858  219.223  219.277]\n",
      "Episode #84 ended! Reward: -31896\n",
      "Eps reduced to 0.4923\n",
      "Episodes: 84. Steps: 350000. Score: -26199.71 Alpha: 1e-05 Eps: 0.49227\n",
      "reward =  -8295.0\n",
      "q =  [-3098.676 -3098.824 -3098.619]\n",
      "w norm =  [ 222.258  222.266  222.252]\n",
      "Episode #85 ended! Reward: -30000\n",
      "Eps reduced to 0.4898\n",
      "Episodes: 85. Steps: 355000. Score: -26244.42 Alpha: 1e-05 Eps: 0.4898\n",
      "reward =  -3248.0\n",
      "q =  [-3135.965 -3138.94  -3137.571]\n",
      "w norm =  [ 224.935  225.147  225.048]\n",
      "Episode #86 ended! Reward: -36075\n",
      "Eps reduced to 0.4874\n",
      "Episodes: 86. Steps: 360000. Score: -26358.73 Alpha: 1e-05 Eps: 0.48736\n",
      "reward =  -544.0\n",
      "q =  [-3181.746 -3182.029 -3181.986]\n",
      "w norm =  [ 228.218  228.238  228.234]\n",
      "Episodes: 86. Steps: 365000. Score: -26358.73 Alpha: 1e-05 Eps: 0.48736\n",
      "reward =  -25377.0\n",
      "q =  [-3226.987 -3238.606 -3227.681]\n",
      "w norm =  [ 231.465  232.296  231.512]\n",
      "Episode #87 ended! Reward: -35869\n",
      "Eps reduced to 0.4849\n",
      "Episode #88 ended! Reward: -28252\n",
      "Eps reduced to 0.4825\n",
      "Episodes: 88. Steps: 370000. Score: -26488.32 Alpha: 1e-05 Eps: 0.48249\n",
      "reward =  -3390.0\n",
      "q =  [-3279.17  -3273.823 -3272.527]\n",
      "w norm =  [ 235.206  234.827  234.728]\n",
      "Episode #89 ended! Reward: -35210\n",
      "Eps reduced to 0.4801\n",
      "Episodes: 89. Steps: 375000. Score: -26586.31 Alpha: 1e-05 Eps: 0.48008\n",
      "reward =  -577.0\n",
      "q =  [-3320.006 -3318.587 -3318.592]\n",
      "w norm =  [ 238.139  238.037  238.032]\n",
      "Episodes: 89. Steps: 380000. Score: -26586.31 Alpha: 1e-05 Eps: 0.48008\n",
      "reward =  -25667.0\n",
      "q =  [-3367.114 -3365.462 -3365.567]\n",
      "w norm =  [ 241.519  241.402  241.402]\n",
      "Episode #90 ended! Reward: -30000\n",
      "Eps reduced to 0.4777\n",
      "Episodes: 90. Steps: 385000. Score: -26624.24 Alpha: 1e-05 Eps: 0.47768\n",
      "reward =  -20681.0\n",
      "q =  [-3416.505 -3411.671 -3411.678]\n",
      "w norm =  [ 245.065  244.717  244.71 ]\n",
      "Episode #91 ended! Reward: -30000\n",
      "Eps reduced to 0.4753\n",
      "Episodes: 91. Steps: 390000. Score: -26661.34 Alpha: 1e-05 Eps: 0.47529\n",
      "reward =  -15652.0\n",
      "q =  [-3446.202 -3446.187 -3445.98 ]\n",
      "w norm =  [ 247.199  247.195  247.173]\n",
      "Episode #92 ended! Reward: -30000\n",
      "Eps reduced to 0.4729\n",
      "Episodes: 92. Steps: 395000. Score: -26697.63 Alpha: 1e-05 Eps: 0.47292\n",
      "reward =  -10562.0\n",
      "q =  [-3467.735 -3467.853 -3467.872]\n",
      "w norm =  [ 248.744  248.753  248.746]\n",
      "Episode #93 ended! Reward: -30000\n",
      "Eps reduced to 0.4706\n",
      "Episodes: 93. Steps: 400000. Score: -26733.14 Alpha: 1e-05 Eps: 0.47055\n",
      "reward =  -5440.0\n",
      "q =  [-3496.622 -3496.777 -3492.418]\n",
      "w norm =  [ 250.816  250.83   250.505]\n",
      "Episode #94 ended! Reward: -30000\n",
      "Eps reduced to 0.4682\n",
      "Episodes: 94. Steps: 405000. Score: -26767.89 Alpha: 1e-05 Eps: 0.4682\n",
      "reward =  -353.0\n",
      "q =  [-3530.669 -3530.883 -3530.602]\n",
      "w norm =  [ 253.256  253.277  253.242]\n",
      "Episode #95 ended! Reward: -24009\n",
      "Eps reduced to 0.4659\n",
      "Episode #96 ended! Reward: -15645\n",
      "Eps reduced to 0.4635\n",
      "Episodes: 96. Steps: 410000. Score: -26623.29 Alpha: 1e-05 Eps: 0.46353\n",
      "reward =  -3150.0\n",
      "q =  [-3575.59  -3575.684 -3575.696]\n",
      "w norm =  [ 256.476  256.496  256.478]\n",
      "Episode #97 ended! Reward: -27508\n",
      "Eps reduced to 0.4612\n",
      "Episode #98 ended! Reward: -14733\n",
      "Eps reduced to 0.4589\n",
      "Episodes: 98. Steps: 415000. Score: -26510.99 Alpha: 1e-05 Eps: 0.45891\n",
      "reward =  -3844.0\n",
      "q =  [-3616.103 -3615.274 -3616.379]\n",
      "w norm =  [ 259.38  259.34  259.4 ]\n",
      "Episodes: 98. Steps: 420000. Score: -26510.99 Alpha: 1e-05 Eps: 0.45891\n",
      "reward =  -29031.0\n",
      "q =  [-3662.373 -3660.329 -3661.056]\n",
      "w norm =  [ 262.697  262.571  262.608]\n",
      "Episode #99 ended! Reward: -30000\n",
      "Eps reduced to 0.4566\n",
      "Episode #100 ended! Reward: -17050\n",
      "Eps reduced to 0.4543\n",
      "Alpha reduced to 0.0000\n",
      "Episodes: 100. Steps: 425000. Score: -26451.27 Alpha: 6.5975e-06 Eps: 0.45433\n",
      "reward =  -16637.0\n",
      "q =  [-3688.498 -3687.874 -3691.837]\n",
      "w norm =  [ 264.57   264.552  264.831]\n",
      "Episode #101 ended! Reward: -30000\n",
      "Eps reduced to 0.4521\n",
      "Episodes: 101. Steps: 430000. Score: -26469.45 Alpha: 6.5975e-06 Eps: 0.45206\n",
      "reward =  -11642.0\n",
      "q =  [-3714.234 -3713.918 -3714.325]\n",
      "w norm =  [ 266.417  266.42   266.446]\n",
      "Episode #102 ended! Reward: -30000\n",
      "Eps reduced to 0.4498\n",
      "Episodes: 102. Steps: 435000. Score: -26582.20 Alpha: 6.5975e-06 Eps: 0.4498\n",
      "reward =  -6518.0\n",
      "q =  [-3734.994 -3738.349 -3738.727]\n",
      "w norm =  [ 267.906  268.171  268.198]\n",
      "Episode #103 ended! Reward: -23388\n",
      "Eps reduced to 0.4475\n",
      "Episode #104 ended! Reward: -19012\n",
      "Eps reduced to 0.4453\n",
      "Episodes: 104. Steps: 440000. Score: -26339.75 Alpha: 6.5975e-06 Eps: 0.44531\n",
      "reward =  -6152.0\n",
      "q =  [-3769.342 -3766.859 -3766.694]\n",
      "w norm =  [ 270.374  270.218  270.207]\n",
      "Episode #105 ended! Reward: -30000\n",
      "Eps reduced to 0.4431\n",
      "Episodes: 105. Steps: 445000. Score: -26263.03 Alpha: 6.5975e-06 Eps: 0.44308\n",
      "reward =  -1105.0\n",
      "q =  [-3796.664 -3792.002 -3792.312]\n",
      "w norm =  [ 272.341  272.021  272.044]\n",
      "Episodes: 105. Steps: 450000. Score: -26263.03 Alpha: 6.5975e-06 Eps: 0.44308\n",
      "reward =  -26103.0\n",
      "q =  [-3819.414 -3818.424 -3817.933]\n",
      "w norm =  [ 273.984  273.916  273.885]\n",
      "Episode #106 ended! Reward: -30000\n",
      "Eps reduced to 0.4409\n",
      "Episodes: 106. Steps: 455000. Score: -26263.03 Alpha: 6.5975e-06 Eps: 0.44087\n",
      "reward =  -21014.0\n",
      "q =  [-3845.654 -3842.888 -3845.908]\n",
      "w norm =  [ 275.876  275.669  275.894]\n",
      "Episode #107 ended! Reward: -30000\n",
      "Eps reduced to 0.4387\n",
      "Episodes: 107. Steps: 460000. Score: -26256.61 Alpha: 6.5975e-06 Eps: 0.43866\n",
      "reward =  -15943.0\n",
      "q =  [-3866.126 -3864.787 -3865.205]\n",
      "w norm =  [ 277.345  277.241  277.28 ]\n",
      "Episode #108 ended! Reward: -30000\n",
      "Eps reduced to 0.4365\n",
      "Episodes: 108. Steps: 465000. Score: -26164.63 Alpha: 6.5975e-06 Eps: 0.43647\n",
      "reward =  -10970.0\n",
      "q =  [-3894.717 -3894.652 -3894.967]\n",
      "w norm =  [ 279.398  279.383  279.416]\n",
      "Episode #109 ended! Reward: -30000\n",
      "Eps reduced to 0.4343\n",
      "Episodes: 109. Steps: 470000. Score: -26271.30 Alpha: 6.5975e-06 Eps: 0.43429\n",
      "reward =  -5819.0\n",
      "q =  [-3919.11  -3919.675 -3920.3  ]\n",
      "w norm =  [ 281.146  281.177  281.232]\n",
      "Episode #110 ended! Reward: -19095\n",
      "Eps reduced to 0.4321\n",
      "Episode #111 ended! Reward: -15146\n",
      "Eps reduced to 0.4300\n",
      "Episodes: 111. Steps: 475000. Score: -26230.79 Alpha: 6.5975e-06 Eps: 0.42996\n",
      "reward =  -12600.0\n",
      "q =  [-3943.897 -3943.192 -3945.844]\n",
      "w norm =  [ 282.926  282.862  283.067]\n",
      "Episode #112 ended! Reward: -28665\n",
      "Eps reduced to 0.4278\n",
      "Episode #113 ended! Reward: -16337\n",
      "Eps reduced to 0.4257\n",
      "Episodes: 113. Steps: 480000. Score: -26215.42 Alpha: 6.5975e-06 Eps: 0.42567\n",
      "reward =  -5525.0\n",
      "q =  [-3969.661 -3969.602 -3976.625]\n",
      "w norm =  [ 284.778  284.757  285.277]\n",
      "Episode #114 ended! Reward: -28370\n",
      "Eps reduced to 0.4235\n",
      "Episodes: 114. Steps: 485000. Score: -26199.12 Alpha: 6.5975e-06 Eps: 0.42354\n",
      "reward =  -11199.0\n",
      "q =  [-3996.009 -3995.615 -3996.604]\n",
      "w norm =  [ 286.669  286.625  286.718]\n",
      "Episode #115 ended! Reward: -23797\n",
      "Eps reduced to 0.4214\n",
      "Episodes: 115. Steps: 490000. Score: -26254.06 Alpha: 6.5975e-06 Eps: 0.42142\n",
      "reward =  -19461.0\n",
      "q =  [-4028.687 -4027.238 -4028.462]\n",
      "w norm =  [ 289.018  288.895  289.011]\n",
      "Episode #116 ended! Reward: -30000\n",
      "Eps reduced to 0.4193\n",
      "Episodes: 116. Steps: 495000. Score: -26365.00 Alpha: 6.5975e-06 Eps: 0.41931\n",
      "reward =  -14298.0\n",
      "q =  [-4055.021 -4060.595 -4055.079]\n",
      "w norm =  [ 290.909  291.29   290.922]\n",
      "Episode #117 ended! Reward: -30000\n",
      "Eps reduced to 0.4172\n",
      "Episodes: 117. Steps: 500000. Score: -26270.58 Alpha: 6.5975e-06 Eps: 0.41722\n",
      "reward =  -9364.0\n",
      "q =  [-4083.18  -4082.98  -4082.681]\n",
      "w norm =  [ 292.932  292.9    292.901]\n",
      "Episode #118 ended! Reward: -30000\n",
      "Eps reduced to 0.4151\n",
      "Episodes: 118. Steps: 505000. Score: -26341.77 Alpha: 6.5975e-06 Eps: 0.41513\n",
      "reward =  -4445.0\n",
      "q =  [-4116.193 -4113.947 -4114.116]\n",
      "w norm =  [ 295.308  295.121  295.157]\n",
      "Episodes: 118. Steps: 510000. Score: -26341.77 Alpha: 6.5975e-06 Eps: 0.41513\n",
      "reward =  -29367.0\n",
      "q =  [-4142.336 -4139.233 -4142.39 ]\n",
      "w norm =  [ 297.183  296.938  297.187]\n",
      "Episode #119 ended! Reward: -30000\n",
      "Eps reduced to 0.4131\n",
      "Episodes: 119. Steps: 515000. Score: -26420.82 Alpha: 6.5975e-06 Eps: 0.41305\n",
      "reward =  -24384.0\n",
      "q =  [-4160.613 -4158.988 -4160.487]\n",
      "w norm =  [ 298.5    298.356  298.484]\n",
      "Episode #120 ended! Reward: -30000\n",
      "Eps reduced to 0.4110\n",
      "Episodes: 120. Steps: 520000. Score: -26420.82 Alpha: 6.5975e-06 Eps: 0.41099\n",
      "reward =  -19318.0\n",
      "q =  [-4179.913 -4179.514 -4179.86 ]\n",
      "w norm =  [ 299.884  299.831  299.874]\n",
      "Episode #121 ended! Reward: -30000\n",
      "Eps reduced to 0.4089\n",
      "Episodes: 121. Steps: 525000. Score: -26420.82 Alpha: 6.5975e-06 Eps: 0.40893\n",
      "reward =  -14336.0\n",
      "q =  [-4199.056 -4201.03  -4201.039]\n",
      "w norm =  [ 301.256  301.374  301.393]\n",
      "Episode #122 ended! Reward: -27525\n",
      "Eps reduced to 0.4069\n",
      "Episode #123 ended! Reward: -28022\n",
      "Eps reduced to 0.4049\n",
      "Episodes: 123. Steps: 530000. Score: -26398.10 Alpha: 6.5975e-06 Eps: 0.40486\n",
      "reward =  -97.0\n",
      "q =  [-4224.797 -4227.689 -4227.033]\n",
      "w norm =  [ 303.101  303.291  303.256]\n",
      "Episode #124 ended! Reward: -17746\n",
      "Eps reduced to 0.4028\n",
      "Episode #125 ended! Reward: -18799\n",
      "Eps reduced to 0.4008\n",
      "Episodes: 125. Steps: 535000. Score: -26274.20 Alpha: 6.5975e-06 Eps: 0.40082\n",
      "reward =  -4488.0\n",
      "q =  [-4243.729 -4242.603 -4242.828]\n",
      "w norm =  [ 304.462  304.366  304.392]\n",
      "Episodes: 125. Steps: 540000. Score: -26274.20 Alpha: 6.5975e-06 Eps: 0.40082\n",
      "reward =  -29349.0\n",
      "q =  [-4279.623 -4272.414 -4272.867]\n",
      "w norm =  [ 307.038  306.504  306.55 ]\n",
      "Episode #126 ended! Reward: -30000\n",
      "Eps reduced to 0.3988\n",
      "Episodes: 126. Steps: 545000. Score: -26253.97 Alpha: 6.5975e-06 Eps: 0.39881\n",
      "reward =  -24492.0\n",
      "q =  [-4304.782 -4298.733 -4301.049]\n",
      "w norm =  [ 308.846  308.395  308.578]\n",
      "Episode #127 ended! Reward: -30000\n",
      "Eps reduced to 0.3968\n",
      "Episodes: 127. Steps: 550000. Score: -26406.28 Alpha: 6.5975e-06 Eps: 0.39682\n",
      "reward =  -19516.0\n",
      "q =  [-4323.817 -4322.481 -4320.483]\n",
      "w norm =  [ 310.217  310.102  309.974]\n",
      "Episode #128 ended! Reward: -30000\n",
      "Eps reduced to 0.3948\n",
      "Episodes: 128. Steps: 555000. Score: -26406.28 Alpha: 6.5975e-06 Eps: 0.39483\n",
      "reward =  -14341.0\n",
      "q =  [-4337.169 -4335.762 -4336.401]\n",
      "w norm =  [ 311.178  311.056  311.116]\n",
      "Episode #129 ended! Reward: -30000\n",
      "Eps reduced to 0.3929\n",
      "Episodes: 129. Steps: 560000. Score: -26406.28 Alpha: 6.5975e-06 Eps: 0.39286\n",
      "reward =  -9324.0\n",
      "q =  [-4366.341 -4365.926 -4366.85 ]\n",
      "w norm =  [ 313.273  313.223  313.3  ]\n",
      "Episode #130 ended! Reward: -30000\n",
      "Eps reduced to 0.3909\n",
      "Episodes: 130. Steps: 565000. Score: -26406.28 Alpha: 6.5975e-06 Eps: 0.3909\n",
      "reward =  -4485.0\n",
      "q =  [-4390.111 -4390.104 -4390.462]\n",
      "w norm =  [ 314.98   314.959  314.994]\n",
      "Episodes: 130. Steps: 570000. Score: -26406.28 Alpha: 6.5975e-06 Eps: 0.3909\n",
      "reward =  -29374.0\n",
      "q =  [-4412.647 -4411.068 -4412.736]\n",
      "w norm =  [ 316.595  316.467  316.59 ]\n",
      "Episode #131 ended! Reward: -30000\n",
      "Eps reduced to 0.3889\n",
      "Episode #132 ended! Reward: -22525\n",
      "Eps reduced to 0.3870\n",
      "Episode #133 ended! Reward: -17040\n",
      "Eps reduced to 0.3851\n",
      "Episodes: 133. Steps: 575000. Score: -26332.71 Alpha: 6.5975e-06 Eps: 0.38506\n",
      "reward =  -2821.0\n",
      "q =  [-4424.48  -4426.841 -4425.693]\n",
      "w norm =  [ 317.443  317.604  317.522]\n",
      "Episode #134 ended! Reward: -17504\n",
      "Eps reduced to 0.3831\n",
      "Episode #135 ended! Reward: -18992\n",
      "Eps reduced to 0.3812\n",
      "Episodes: 135. Steps: 580000. Score: -26097.67 Alpha: 6.5975e-06 Eps: 0.38122\n",
      "reward =  -8322.0\n",
      "q =  [-4448.641 -4452.988 -4444.02 ]\n",
      "w norm =  [ 319.176  319.495  318.844]\n",
      "Episode #136 ended! Reward: -30000\n",
      "Eps reduced to 0.3793\n",
      "Episodes: 136. Steps: 585000. Score: -26218.55 Alpha: 6.5975e-06 Eps: 0.37932\n",
      "reward =  -3123.0\n",
      "q =  [-4482.748 -4481.873 -4476.578]\n",
      "w norm =  [ 321.626  321.571  321.185]\n",
      "Episodes: 136. Steps: 590000. Score: -26218.55 Alpha: 6.5975e-06 Eps: 0.37932\n",
      "reward =  -28181.0\n",
      "q =  [-4502.847 -4507.046 -4502.798]\n",
      "w norm =  [ 323.068  323.382  323.071]\n",
      "Episode #137 ended! Reward: -30000\n",
      "Eps reduced to 0.3774\n",
      "Episodes: 137. Steps: 595000. Score: -26377.69 Alpha: 6.5975e-06 Eps: 0.37742\n",
      "reward =  -23104.0\n",
      "q =  [-4530.971 -4529.842 -4530.192]\n",
      "w norm =  [ 325.091  325.019  325.041]\n",
      "Episode #138 ended! Reward: -30000\n",
      "Eps reduced to 0.3755\n",
      "Episodes: 138. Steps: 600000. Score: -26492.80 Alpha: 6.5975e-06 Eps: 0.37553\n",
      "reward =  -18118.0\n",
      "q =  [-4559.443 -4558.727 -4558.655]\n",
      "w norm =  [ 327.14   327.089  327.081]\n",
      "Episode #139 ended! Reward: -30000\n",
      "Eps reduced to 0.3737\n",
      "Episode #140 ended! Reward: -22250\n",
      "Eps reduced to 0.3718\n",
      "Episodes: 140. Steps: 605000. Score: -26415.30 Alpha: 6.5975e-06 Eps: 0.37179\n",
      "reward =  -280.0\n",
      "q =  [-4571.709 -4575.342 -4575.383]\n",
      "w norm =  [ 328.024  328.28   328.283]\n",
      "Episode #141 ended! Reward: -13846\n",
      "Eps reduced to 0.3699\n",
      "Episodes: 141. Steps: 610000. Score: -26293.40 Alpha: 6.5975e-06 Eps: 0.36993\n",
      "reward =  -18375.0\n",
      "q =  [-4596.059 -4594.681 -4590.126]\n",
      "w norm =  [ 329.779  329.673  329.344]\n",
      "Episode #142 ended! Reward: -30000\n",
      "Eps reduced to 0.3681\n",
      "Episodes: 142. Steps: 615000. Score: -26293.40 Alpha: 6.5975e-06 Eps: 0.36808\n",
      "reward =  -13301.0\n",
      "q =  [-4611.515 -4609.176 -4603.496]\n",
      "w norm =  [ 330.895  330.715  330.308]\n",
      "Episode #143 ended! Reward: -30000\n",
      "Eps reduced to 0.3662\n",
      "Episodes: 143. Steps: 620000. Score: -26293.40 Alpha: 6.5975e-06 Eps: 0.36624\n",
      "reward =  -8230.0\n",
      "q =  [-4624.987 -4624.548 -4624.98 ]\n",
      "w norm =  [ 331.869  331.826  331.852]\n",
      "Episode #144 ended! Reward: -30000\n",
      "Eps reduced to 0.3644\n",
      "Episodes: 144. Steps: 625000. Score: -26293.40 Alpha: 6.5975e-06 Eps: 0.36441\n",
      "reward =  -3042.0\n",
      "q =  [-4632.798 -4625.417 -4632.029]\n",
      "w norm =  [ 332.433  331.891  332.365]\n",
      "Episodes: 144. Steps: 630000. Score: -26293.40 Alpha: 6.5975e-06 Eps: 0.36441\n",
      "reward =  -27830.0\n",
      "q =  [-4652.735 -4642.908 -4653.164]\n",
      "w norm =  [ 333.861  333.147  333.89 ]\n",
      "Episode #145 ended! Reward: -30000\n",
      "Eps reduced to 0.3626\n",
      "Episodes: 145. Steps: 635000. Score: -26404.93 Alpha: 6.5975e-06 Eps: 0.36258\n",
      "reward =  -22680.0\n",
      "q =  [-4655.863 -4656.083 -4655.771]\n",
      "w norm =  [ 334.087  334.095  334.079]\n",
      "Episode #146 ended! Reward: -30000\n",
      "Eps reduced to 0.3608\n",
      "Episodes: 146. Steps: 640000. Score: -26485.07 Alpha: 6.5975e-06 Eps: 0.36077\n",
      "reward =  -17652.0\n",
      "q =  [-4665.316 -4663.546 -4664.702]\n",
      "w norm =  [ 334.771  334.632  334.722]\n",
      "Episode #147 ended! Reward: -30000\n",
      "Eps reduced to 0.3590\n",
      "Episode #148 ended! Reward: -17924\n",
      "Eps reduced to 0.3572\n",
      "Episodes: 148. Steps: 645000. Score: -26513.73 Alpha: 6.5975e-06 Eps: 0.35717\n",
      "reward =  -3999.0\n",
      "q =  [-4693.889 -4693.655 -4693.728]\n",
      "w norm =  [ 336.821  336.795  336.806]\n",
      "Episode #149 ended! Reward: -19230\n",
      "Eps reduced to 0.3554\n",
      "Episodes: 149. Steps: 650000. Score: -26444.92 Alpha: 6.5975e-06 Eps: 0.35539\n",
      "reward =  -18313.0\n",
      "q =  [-4711.801 -4715.139 -4722.144]\n",
      "w norm =  [ 338.115  338.345  338.845]\n",
      "Episode #150 ended! Reward: -34492\n",
      "Eps reduced to 0.3536\n",
      "Episode #151 ended! Reward: -12946\n",
      "Eps reduced to 0.3518\n",
      "Episodes: 151. Steps: 655000. Score: -26605.64 Alpha: 6.5975e-06 Eps: 0.35184\n",
      "reward =  -12778.0\n",
      "q =  [-4742.905 -4741.229 -4741.066]\n",
      "w norm =  [ 340.347  340.222  340.207]\n",
      "Episode #152 ended! Reward: -30000\n",
      "Eps reduced to 0.3501\n",
      "Episodes: 152. Steps: 660000. Score: -26701.08 Alpha: 6.5975e-06 Eps: 0.35008\n",
      "reward =  -7874.0\n",
      "q =  [-4766.412 -4761.788 -4766.25 ]\n",
      "w norm =  [ 342.033  341.706  342.016]\n",
      "Episode #153 ended! Reward: -30000\n",
      "Eps reduced to 0.3483\n",
      "Episodes: 153. Steps: 665000. Score: -26829.88 Alpha: 6.5975e-06 Eps: 0.34833\n",
      "reward =  -2891.0\n",
      "q =  [-4784.862 -4778.796 -4784.676]\n",
      "w norm =  [ 343.355  342.93   343.343]\n",
      "Episodes: 153. Steps: 670000. Score: -26829.88 Alpha: 6.5975e-06 Eps: 0.34833\n",
      "reward =  -27921.0\n",
      "q =  [-4815.054 -4810.519 -4810.945]\n",
      "w norm =  [ 345.519  345.207  345.239]\n",
      "Episode #154 ended! Reward: -30000\n",
      "Eps reduced to 0.3466\n",
      "Episodes: 154. Steps: 675000. Score: -26969.11 Alpha: 6.5975e-06 Eps: 0.34659\n",
      "reward =  -22781.0\n",
      "q =  [-4828.074 -4823.345 -4827.876]\n",
      "w norm =  [ 346.456  346.129  346.467]\n",
      "Episode #155 ended! Reward: -30000\n",
      "Eps reduced to 0.3449\n",
      "Episodes: 155. Steps: 680000. Score: -26940.18 Alpha: 6.5975e-06 Eps: 0.34486\n",
      "reward =  -17714.0\n",
      "q =  [-4854.025 -4853.343 -4853.396]\n",
      "w norm =  [ 348.319  348.279  348.303]\n",
      "Episode #156 ended! Reward: -30000\n",
      "Eps reduced to 0.3431\n",
      "Episodes: 156. Steps: 685000. Score: -26883.90 Alpha: 6.5975e-06 Eps: 0.34313\n",
      "reward =  -12567.0\n",
      "q =  [-4871.176 -4871.393 -4871.842]\n",
      "w norm =  [ 349.556  349.575  349.635]\n",
      "Episode #157 ended! Reward: -34152\n",
      "Eps reduced to 0.3414\n",
      "Episode #158 ended! Reward: -18850\n",
      "Eps reduced to 0.3397\n",
      "Episodes: 158. Steps: 690000. Score: -26813.92 Alpha: 6.5975e-06 Eps: 0.33971\n",
      "reward =  -2967.0\n",
      "q =  [-4871.189 -4872.692 -4869.198]\n",
      "w norm =  [ 349.559  349.668  349.447]\n",
      "Episode #159 ended! Reward: -35834\n",
      "Eps reduced to 0.3380\n",
      "Episodes: 159. Steps: 695000. Score: -26872.26 Alpha: 6.5975e-06 Eps: 0.33801\n",
      "reward =  -1536.0\n",
      "q =  [-4884.95  -4885.692 -4885.724]\n",
      "w norm =  [ 350.557  350.604  350.636]\n",
      "Episode #160 ended! Reward: -16768\n",
      "Eps reduced to 0.3363\n",
      "Episode #161 ended! Reward: -18647\n",
      "Eps reduced to 0.3346\n",
      "Episodes: 161. Steps: 700000. Score: -26626.41 Alpha: 6.5975e-06 Eps: 0.33464\n",
      "reward =  -8056.0\n",
      "q =  [-4915.223 -4914.948 -4915.08 ]\n",
      "w norm =  [ 352.731  352.71   352.75 ]\n",
      "Episode #162 ended! Reward: -30000\n",
      "Eps reduced to 0.3330\n",
      "Episodes: 162. Steps: 705000. Score: -26626.41 Alpha: 6.5975e-06 Eps: 0.33297\n",
      "reward =  -2974.0\n",
      "q =  [-4946.294 -4943.549 -4941.863]\n",
      "w norm =  [ 354.962  354.783  354.671]\n",
      "Episodes: 162. Steps: 710000. Score: -26626.41 Alpha: 6.5975e-06 Eps: 0.33297\n",
      "reward =  -27940.0\n",
      "q =  [-4975.775 -4975.565 -4971.323]\n",
      "w norm =  [ 357.079  357.095  356.784]\n",
      "Episode #163 ended! Reward: -30000\n",
      "Eps reduced to 0.3313\n",
      "Episodes: 163. Steps: 715000. Score: -26714.41 Alpha: 6.5975e-06 Eps: 0.3313\n",
      "reward =  -22854.0\n",
      "q =  [-4991.105 -4989.28  -4988.423]\n",
      "w norm =  [ 358.182  358.094  358.013]\n",
      "Episode #164 ended! Reward: -30000\n",
      "Eps reduced to 0.3296\n",
      "Episodes: 164. Steps: 720000. Score: -26680.21 Alpha: 6.5975e-06 Eps: 0.32965\n",
      "reward =  -17767.0\n",
      "q =  [-5019.334 -5019.271 -5018.166]\n",
      "w norm =  [ 360.214  360.261  360.145]\n",
      "Episode #165 ended! Reward: -30000\n",
      "Eps reduced to 0.3280\n",
      "Episodes: 165. Steps: 725000. Score: -26804.96 Alpha: 6.5975e-06 Eps: 0.328\n",
      "reward =  -12702.0\n",
      "q =  [-5036.026 -5034.355 -5034.424]\n",
      "w norm =  [ 361.419  361.345  361.312]\n",
      "Episode #166 ended! Reward: -30000\n",
      "Eps reduced to 0.3264\n",
      "Episodes: 166. Steps: 730000. Score: -26890.15 Alpha: 6.5975e-06 Eps: 0.32636\n",
      "reward =  -7552.0\n",
      "q =  [-5063.548 -5063.166 -5063.521]\n",
      "w norm =  [ 363.405  363.411  363.4  ]\n",
      "Episode #167 ended! Reward: -30000\n",
      "Eps reduced to 0.3247\n",
      "Episodes: 167. Steps: 735000. Score: -27039.13 Alpha: 6.5975e-06 Eps: 0.32473\n",
      "reward =  -2699.0\n",
      "q =  [-5086.535 -5086.115 -5085.659]\n",
      "w norm =  [ 365.068  365.057  364.986]\n",
      "Episodes: 167. Steps: 740000. Score: -27039.13 Alpha: 6.5975e-06 Eps: 0.32473\n",
      "reward =  -27562.0\n",
      "q =  [-5105.52  -5105.671 -5101.411]\n",
      "w norm =  [ 366.437  366.46   366.114]\n",
      "Episode #168 ended! Reward: -30000\n",
      "Eps reduced to 0.3231\n",
      "Episode #169 ended! Reward: -30807\n",
      "Eps reduced to 0.3215\n",
      "Episodes: 169. Steps: 745000. Score: -27180.50 Alpha: 6.5975e-06 Eps: 0.32149\n",
      "reward =  -1772.0\n",
      "q =  [-5132.583 -5132.636 -5130.044]\n",
      "w norm =  [ 368.381  368.399  368.171]\n",
      "Episodes: 169. Steps: 750000. Score: -27180.50 Alpha: 6.5975e-06 Eps: 0.32149\n",
      "reward =  -26725.0\n",
      "q =  [-5147.516 -5147.76  -5148.276]\n",
      "w norm =  [ 369.451  369.484  369.484]\n",
      "Episode #170 ended! Reward: -30000\n",
      "Eps reduced to 0.3199\n",
      "Episodes: 170. Steps: 755000. Score: -27180.50 Alpha: 6.5975e-06 Eps: 0.31988\n",
      "reward =  -21876.0\n",
      "q =  [-5167.882 -5166.46  -5168.326]\n",
      "w norm =  [ 370.91   370.831  370.931]\n",
      "Episode #171 ended! Reward: -30000\n",
      "Eps reduced to 0.3183\n",
      "Episodes: 171. Steps: 760000. Score: -27102.96 Alpha: 6.5975e-06 Eps: 0.31828\n",
      "reward =  -16607.0\n",
      "q =  [-5182.015 -5181.743 -5187.053]\n",
      "w norm =  [ 371.924  371.926  372.281]\n",
      "Episode #172 ended! Reward: -30000\n",
      "Eps reduced to 0.3167\n",
      "Episode #173 ended! Reward: -18975\n",
      "Eps reduced to 0.3151\n",
      "Episodes: 173. Steps: 765000. Score: -27036.86 Alpha: 6.5975e-06 Eps: 0.3151\n",
      "reward =  -2155.0\n",
      "q =  [-5197.563 -5199.121 -5199.471]\n",
      "w norm =  [ 373.042  373.172  373.176]\n",
      "Episodes: 173. Steps: 770000. Score: -27036.86 Alpha: 6.5975e-06 Eps: 0.3151\n",
      "reward =  -27192.0\n",
      "q =  [-5207.181 -5205.324 -5209.485]\n",
      "w norm =  [ 373.74   373.618  373.907]\n",
      "Episode #174 ended! Reward: -30000\n",
      "Eps reduced to 0.3135\n",
      "Episode #175 ended! Reward: -12401\n",
      "Eps reduced to 0.3120\n",
      "Episodes: 175. Steps: 775000. Score: -27093.03 Alpha: 6.5975e-06 Eps: 0.31196\n",
      "reward =  -15304.0\n",
      "q =  [-5214.835 -5211.34  -5214.953]\n",
      "w norm =  [ 374.302  374.052  374.312]\n",
      "Episode #176 ended! Reward: -30000\n",
      "Eps reduced to 0.3104\n",
      "Episodes: 176. Steps: 780000. Score: -27093.03 Alpha: 6.5975e-06 Eps: 0.3104\n",
      "reward =  -10039.0\n",
      "q =  [-5217.5   -5217.787 -5218.348]\n",
      "w norm =  [ 374.502  374.514  374.566]\n",
      "Episode #177 ended! Reward: -21376\n",
      "Eps reduced to 0.3088\n",
      "Episodes: 177. Steps: 785000. Score: -27142.36 Alpha: 6.5975e-06 Eps: 0.30885\n",
      "reward =  -22533.0\n",
      "q =  [-5237.197 -5236.668 -5232.563]\n",
      "w norm =  [ 375.922  375.873  375.584]\n",
      "Episode #178 ended! Reward: -30000\n",
      "Eps reduced to 0.3073\n",
      "Episodes: 178. Steps: 790000. Score: -27142.36 Alpha: 6.5975e-06 Eps: 0.30731\n",
      "reward =  -17494.0\n",
      "q =  [-5242.91  -5244.259 -5241.299]\n",
      "w norm =  [ 376.336  376.429  376.21 ]\n",
      "Episode #179 ended! Reward: -30000\n",
      "Eps reduced to 0.3058\n",
      "Episodes: 179. Steps: 795000. Score: -27142.36 Alpha: 6.5975e-06 Eps: 0.30577\n",
      "reward =  -12356.0\n",
      "q =  [-5267.163 -5264.234 -5267.636]\n",
      "w norm =  [ 378.081  377.87   378.099]\n",
      "Episode #180 ended! Reward: -30000\n",
      "Eps reduced to 0.3042\n",
      "Episodes: 180. Steps: 800000. Score: -27142.36 Alpha: 6.5975e-06 Eps: 0.30424\n",
      "reward =  -7382.0\n",
      "q =  [-5291.041 -5289.899 -5288.786]\n",
      "w norm =  [ 379.806  379.726  379.618]\n",
      "Episode #181 ended! Reward: -30000\n",
      "Eps reduced to 0.3027\n",
      "Episodes: 181. Steps: 805000. Score: -27095.87 Alpha: 6.5975e-06 Eps: 0.30272\n",
      "reward =  -2348.0\n",
      "q =  [-5314.416 -5314.034 -5311.105]\n",
      "w norm =  [ 381.483  381.456  381.225]\n",
      "Episodes: 181. Steps: 810000. Score: -27095.87 Alpha: 6.5975e-06 Eps: 0.30272\n",
      "reward =  -27269.0\n",
      "q =  [-5321.661 -5316.97  -5319.975]\n",
      "w norm =  [ 382.002  381.671  381.868]\n",
      "Episode #182 ended! Reward: -30000\n",
      "Eps reduced to 0.3012\n",
      "Episode #183 ended! Reward: -22007\n",
      "Eps reduced to 0.2997\n",
      "Episodes: 183. Steps: 815000. Score: -26987.15 Alpha: 6.5975e-06 Eps: 0.2997\n",
      "reward =  -9667.0\n",
      "q =  [-5343.722 -5341.256 -5343.091]\n",
      "w norm =  [ 383.585  383.415  383.535]\n",
      "Episode #184 ended! Reward: -30000\n",
      "Eps reduced to 0.2982\n",
      "Episodes: 184. Steps: 820000. Score: -26968.19 Alpha: 6.5975e-06 Eps: 0.2982\n",
      "reward =  -4570.0\n",
      "q =  [-5349.597 -5344.865 -5346.375]\n",
      "w norm =  [ 384.006  383.68   383.772]\n",
      "Episodes: 184. Steps: 825000. Score: -26968.19 Alpha: 6.5975e-06 Eps: 0.2982\n",
      "reward =  -29700.0\n",
      "q =  [-5369.826 -5369.684 -5369.989]\n",
      "w norm =  [ 385.455  385.459  385.477]\n",
      "Episode #185 ended! Reward: -30000\n",
      "Eps reduced to 0.2967\n",
      "Episode #186 ended! Reward: -20303\n",
      "Eps reduced to 0.2952\n",
      "Episodes: 186. Steps: 830000. Score: -26810.47 Alpha: 6.5975e-06 Eps: 0.29523\n",
      "reward =  -13971.0\n",
      "q =  [-5394.089 -5394.087 -5392.398]\n",
      "w norm =  [ 387.195  387.209  387.091]\n",
      "Episode #187 ended! Reward: -30000\n",
      "Eps reduced to 0.2937\n",
      "Episodes: 187. Steps: 835000. Score: -26751.78 Alpha: 6.5975e-06 Eps: 0.29375\n",
      "reward =  -8928.0\n",
      "q =  [-5425.2   -5422.234 -5422.198]\n",
      "w norm =  [ 389.431  389.227  389.232]\n",
      "Episode #188 ended! Reward: -30000\n",
      "Eps reduced to 0.2923\n",
      "Episodes: 188. Steps: 840000. Score: -26769.26 Alpha: 6.5975e-06 Eps: 0.29228\n",
      "reward =  -3812.0\n",
      "q =  [-5442.073 -5440.959 -5440.873]\n",
      "w norm =  [ 390.671  390.57   390.571]\n",
      "Episodes: 188. Steps: 845000. Score: -26769.26 Alpha: 6.5975e-06 Eps: 0.29228\n",
      "reward =  -28840.0\n",
      "q =  [-5465.883 -5463.643 -5462.669]\n",
      "w norm =  [ 392.397  392.203  392.132]\n",
      "Episode #189 ended! Reward: -30000\n",
      "Eps reduced to 0.2908\n",
      "Episodes: 189. Steps: 850000. Score: -26717.16 Alpha: 6.5975e-06 Eps: 0.29082\n",
      "reward =  -23596.0\n",
      "q =  [-5487.937 -5486.76  -5487.461]\n",
      "w norm =  [ 393.984  393.867  393.908]\n",
      "Episode #190 ended! Reward: -30000\n",
      "Eps reduced to 0.2894\n",
      "Episodes: 190. Steps: 855000. Score: -26717.16 Alpha: 6.5975e-06 Eps: 0.28937\n",
      "reward =  -18396.0\n",
      "q =  [-5495.503 -5496.231 -5494.59 ]\n",
      "w norm =  [ 394.539  394.566  394.419]\n",
      "Episode #191 ended! Reward: -30000\n",
      "Eps reduced to 0.2879\n",
      "Episodes: 191. Steps: 860000. Score: -26717.16 Alpha: 6.5975e-06 Eps: 0.28792\n",
      "reward =  -13326.0\n",
      "q =  [-5503.099 -5501.7   -5498.191]\n",
      "w norm =  [ 395.086  394.971  394.679]\n",
      "Episode #192 ended! Reward: -30000\n",
      "Eps reduced to 0.2865\n",
      "Episodes: 192. Steps: 865000. Score: -26717.16 Alpha: 6.5975e-06 Eps: 0.28648\n",
      "reward =  -8374.0\n",
      "q =  [-5513.905 -5513.334 -5514.06 ]\n",
      "w norm =  [ 395.864  395.804  395.821]\n",
      "Episode #193 ended! Reward: -30000\n",
      "Eps reduced to 0.2850\n",
      "Episodes: 193. Steps: 870000. Score: -26717.16 Alpha: 6.5975e-06 Eps: 0.28505\n",
      "reward =  -3233.0\n",
      "q =  [-5533.55  -5533.636 -5528.569]\n",
      "w norm =  [ 397.271  397.258  396.87 ]\n",
      "Episode #194 ended! Reward: -19472\n",
      "Eps reduced to 0.2836\n",
      "Episode #195 ended! Reward: -14285\n",
      "Eps reduced to 0.2822\n",
      "Episodes: 195. Steps: 875000. Score: -26514.64 Alpha: 6.5975e-06 Eps: 0.2822\n",
      "reward =  -10422.0\n",
      "q =  [-5546.46  -5542.325 -5546.591]\n",
      "w norm =  [ 398.195  397.88   398.18 ]\n",
      "Episode #196 ended! Reward: -30000\n",
      "Eps reduced to 0.2808\n",
      "Episodes: 196. Steps: 880000. Score: -26658.19 Alpha: 6.5975e-06 Eps: 0.28079\n",
      "reward =  -5408.0\n",
      "q =  [-5565.769 -5565.719 -5568.856]\n",
      "w norm =  [ 399.578  399.556  399.805]\n",
      "Episode #197 ended! Reward: -30000\n",
      "Eps reduced to 0.2794\n",
      "Episodes: 197. Steps: 885000. Score: -26683.11 Alpha: 6.5975e-06 Eps: 0.27939\n",
      "reward =  -335.0\n",
      "q =  [-5586.843 -5586.79  -5588.838]\n",
      "w norm =  [ 401.09   401.065  401.268]\n",
      "Episode #198 ended! Reward: -18414\n",
      "Eps reduced to 0.2780\n",
      "Episode #199 ended! Reward: -22204\n",
      "Eps reduced to 0.2766\n",
      "Episodes: 199. Steps: 890000. Score: -26641.96 Alpha: 6.5975e-06 Eps: 0.2766\n",
      "reward =  -2669.0\n",
      "q =  [-5602.553 -5604.097 -5603.956]\n",
      "w norm =  [ 402.223  402.309  402.366]\n",
      "Episodes: 199. Steps: 895000. Score: -26641.96 Alpha: 6.5975e-06 Eps: 0.2766\n",
      "reward =  -27700.0\n",
      "q =  [-5623.391 -5621.487 -5625.51 ]\n",
      "w norm =  [ 403.72   403.565  403.936]\n",
      "Episode #200 ended! Reward: -30000\n",
      "Eps reduced to 0.2752\n",
      "Alpha reduced to 0.0000\n",
      "Episode #201 ended! Reward: -21288\n",
      "Eps reduced to 0.2738\n",
      "Episodes: 201. Steps: 900000. Score: -26684.34 Alpha: 5.1728e-06 Eps: 0.27384\n",
      "reward =  -10846.0\n",
      "q =  [-5648.229 -5647.355 -5647.025]\n",
      "w norm =  [ 405.506  405.426  405.485]\n",
      "Episode #202 ended! Reward: -30000\n",
      "Eps reduced to 0.2725\n",
      "Episodes: 202. Steps: 905000. Score: -26684.34 Alpha: 5.1728e-06 Eps: 0.27247\n",
      "reward =  -5814.0\n",
      "q =  [-5666.803 -5657.011 -5661.479]\n",
      "w norm =  [ 406.839  406.129  406.521]\n",
      "Episode #203 ended! Reward: -30000\n",
      "Eps reduced to 0.2711\n",
      "Episodes: 203. Steps: 910000. Score: -26750.46 Alpha: 5.1728e-06 Eps: 0.27111\n",
      "reward =  -790.0\n",
      "q =  [-5681.659 -5680.927 -5680.626]\n",
      "w norm =  [ 407.904  407.847  407.892]\n",
      "Episodes: 203. Steps: 915000. Score: -26750.46 Alpha: 5.1728e-06 Eps: 0.27111\n",
      "reward =  -25678.0\n",
      "q =  [-5691.321 -5688.315 -5690.965]\n",
      "w norm =  [ 408.601  408.382  408.633]\n",
      "Episode #204 ended! Reward: -30000\n",
      "Eps reduced to 0.2698\n",
      "Episode #205 ended! Reward: -23264\n",
      "Eps reduced to 0.2684\n",
      "Episodes: 205. Steps: 920000. Score: -26792.98 Alpha: 5.1728e-06 Eps: 0.26841\n",
      "reward =  -6347.0\n",
      "q =  [-5709.501 -5706.906 -5706.809]\n",
      "w norm =  [ 409.912  409.721  409.767]\n",
      "Episode #206 ended! Reward: -30000\n",
      "Eps reduced to 0.2671\n",
      "Episodes: 206. Steps: 925000. Score: -26792.98 Alpha: 5.1728e-06 Eps: 0.26706\n",
      "reward =  -1318.0\n",
      "q =  [-5724.149 -5723.212 -5723.267]\n",
      "w norm =  [ 410.967  410.894  410.946]\n",
      "Episodes: 206. Steps: 930000. Score: -26792.98 Alpha: 5.1728e-06 Eps: 0.26706\n",
      "reward =  -26575.0\n",
      "q =  [-5737.799 -5737.282 -5737.882]\n",
      "w norm =  [ 411.95   411.918  411.992]\n",
      "Episode #207 ended! Reward: -30000\n",
      "Eps reduced to 0.2657\n",
      "Episodes: 207. Steps: 935000. Score: -26792.98 Alpha: 5.1728e-06 Eps: 0.26573\n",
      "reward =  -21501.0\n",
      "q =  [-5755.551 -5754.034 -5755.522]\n",
      "w norm =  [ 413.225  413.119  413.255]\n",
      "Episode #208 ended! Reward: -30000\n",
      "Eps reduced to 0.2644\n",
      "Episodes: 208. Steps: 940000. Score: -26792.98 Alpha: 5.1728e-06 Eps: 0.2644\n",
      "reward =  -16591.0\n",
      "q =  [-5774.336 -5773.202 -5772.788]\n",
      "w norm =  [ 414.578  414.503  414.492]\n",
      "Episode #209 ended! Reward: -30000\n",
      "Eps reduced to 0.2631\n",
      "Episodes: 209. Steps: 945000. Score: -26792.98 Alpha: 5.1728e-06 Eps: 0.26308\n",
      "reward =  -11494.0\n",
      "q =  [-5792.009 -5791.983 -5783.449]\n",
      "w norm =  [ 415.848  415.855  415.255]\n",
      "Episode #210 ended! Reward: -30000\n",
      "Eps reduced to 0.2618\n",
      "Episodes: 210. Steps: 950000. Score: -26902.03 Alpha: 5.1728e-06 Eps: 0.26176\n",
      "reward =  -6473.0\n",
      "q =  [-5795.368 -5794.368 -5793.898]\n",
      "w norm =  [ 416.113  416.028  416.003]\n",
      "Episode #211 ended! Reward: -30000\n",
      "Eps reduced to 0.2605\n",
      "Episodes: 211. Steps: 955000. Score: -27050.57 Alpha: 5.1728e-06 Eps: 0.26045\n",
      "reward =  -1471.0\n",
      "q =  [-5807.778 -5805.344 -5807.782]\n",
      "w norm =  [ 417.016  416.82   416.998]\n",
      "Episodes: 211. Steps: 960000. Score: -27050.57 Alpha: 5.1728e-06 Eps: 0.26045\n",
      "reward =  -26515.0\n",
      "q =  [-5829.519 -5829.676 -5828.86 ]\n",
      "w norm =  [ 418.587  418.576  418.507]\n",
      "Episode #212 ended! Reward: -30000\n",
      "Eps reduced to 0.2592\n",
      "Episode #213 ended! Reward: -31257\n",
      "Eps reduced to 0.2579\n",
      "Episodes: 213. Steps: 965000. Score: -27213.12 Alpha: 5.1728e-06 Eps: 0.25786\n",
      "reward =  -191.0\n",
      "q =  [-5851.869 -5852.327 -5852.08 ]\n",
      "w norm =  [ 420.192  420.216  420.174]\n",
      "Episodes: 213. Steps: 970000. Score: -27213.12 Alpha: 5.1728e-06 Eps: 0.25786\n",
      "reward =  -25044.0\n",
      "q =  [-5847.667 -5848.567 -5845.139]\n",
      "w norm =  [ 419.892  419.97   419.679]\n",
      "Episode #214 ended! Reward: -30000\n",
      "Eps reduced to 0.2566\n",
      "Episodes: 214. Steps: 975000. Score: -27229.42 Alpha: 5.1728e-06 Eps: 0.25657\n",
      "reward =  -20014.0\n",
      "q =  [-5868.13  -5868.68  -5868.161]\n",
      "w norm =  [ 421.357  421.426  421.332]\n",
      "Episode #215 ended! Reward: -30000\n",
      "Eps reduced to 0.2553\n",
      "Episode #216 ended! Reward: -18885\n",
      "Eps reduced to 0.2540\n",
      "Episodes: 216. Steps: 980000. Score: -27180.30 Alpha: 5.1728e-06 Eps: 0.25401\n",
      "reward =  -5683.0\n",
      "q =  [-5873.106 -5879.547 -5878.634]\n",
      "w norm =  [ 421.718  422.214  422.09 ]\n",
      "Episode #217 ended! Reward: -16954\n",
      "Eps reduced to 0.2527\n",
      "Episode #218 ended! Reward: -14083\n",
      "Eps reduced to 0.2515\n",
      "Episodes: 218. Steps: 985000. Score: -26890.67 Alpha: 5.1728e-06 Eps: 0.25147\n",
      "reward =  -14985.0\n",
      "q =  [-5887.605 -5891.312 -5889.373]\n",
      "w norm =  [ 422.76   423.073  422.871]\n",
      "Episode #219 ended! Reward: -25107\n",
      "Eps reduced to 0.2502\n",
      "Episode #220 ended! Reward: -22408\n",
      "Eps reduced to 0.2490\n",
      "Episode #221 ended! Reward: -14944\n",
      "Eps reduced to 0.2477\n",
      "Episodes: 221. Steps: 990000. Score: -26615.26 Alpha: 5.1728e-06 Eps: 0.24772\n",
      "reward =  -516.0\n",
      "q =  [-5894.22  -5899.533 -5896.406]\n",
      "w norm =  [ 423.239  423.667  423.379]\n",
      "Episode #222 ended! Reward: -30062\n",
      "Eps reduced to 0.2465\n",
      "Episodes: 222. Steps: 995000. Score: -26640.63 Alpha: 5.1728e-06 Eps: 0.24648\n",
      "reward =  -4405.0\n",
      "q =  [-5919.625 -5924.8   -5919.166]\n",
      "w norm =  [ 425.064  425.488  425.017]\n",
      "Episodes: 222. Steps: 1000000. Score: -26640.63 Alpha: 5.1728e-06 Eps: 0.24648\n",
      "reward =  -29487.0\n",
      "q =  [-5943.323 -5945.512 -5942.368]\n",
      "w norm =  [ 426.767  426.992  426.68 ]\n",
      "Episode #223 ended! Reward: -30000\n",
      "Eps reduced to 0.2452\n",
      "Episodes: 223. Steps: 1005000. Score: -26660.41 Alpha: 5.1728e-06 Eps: 0.24525\n",
      "reward =  -24490.0\n",
      "q =  [-5967.386 -5966.413 -5966.703]\n",
      "w norm =  [ 428.5    428.491  428.436]\n",
      "Episode #224 ended! Reward: -30000\n",
      "Eps reduced to 0.2440\n",
      "Episodes: 224. Steps: 1010000. Score: -26782.95 Alpha: 5.1728e-06 Eps: 0.24402\n",
      "reward =  -19528.0\n",
      "q =  [-5992.134 -5990.456 -5991.989]\n",
      "w norm =  [ 430.274  430.216  430.26 ]\n",
      "Episode #225 ended! Reward: -30000\n",
      "Eps reduced to 0.2428\n",
      "Episodes: 225. Steps: 1015000. Score: -26894.96 Alpha: 5.1728e-06 Eps: 0.2428\n",
      "reward =  -14582.0\n",
      "q =  [-6012.877 -6011.951 -6007.265]\n",
      "w norm =  [ 431.761  431.755  431.365]\n",
      "Episode #226 ended! Reward: -30000\n",
      "Eps reduced to 0.2416\n",
      "Episodes: 226. Steps: 1020000. Score: -26894.96 Alpha: 5.1728e-06 Eps: 0.24159\n",
      "reward =  -9484.0\n",
      "q =  [-6022.589 -6022.476 -6022.529]\n",
      "w norm =  [ 432.458  432.509  432.467]\n",
      "Episode #227 ended! Reward: -30000\n",
      "Eps reduced to 0.2404\n",
      "Episodes: 227. Steps: 1025000. Score: -26894.96 Alpha: 5.1728e-06 Eps: 0.24038\n",
      "reward =  -4525.0\n",
      "q =  [-6046.461 -6044.618 -6045.871]\n",
      "w norm =  [ 434.181  434.094  434.147]\n",
      "Episodes: 227. Steps: 1030000. Score: -26894.96 Alpha: 5.1728e-06 Eps: 0.24038\n",
      "reward =  -29605.0\n",
      "q =  [-6043.107 -6043.807 -6038.392]\n",
      "w norm =  [ 433.953  434.036  433.619]\n",
      "Episode #228 ended! Reward: -30000\n",
      "Eps reduced to 0.2392\n",
      "Episodes: 228. Steps: 1035000. Score: -26894.96 Alpha: 5.1728e-06 Eps: 0.23918\n",
      "reward =  -24478.0\n",
      "q =  [-6051.695 -6051.102 -6050.436]\n",
      "w norm =  [ 434.58   434.558  434.487]\n",
      "Episode #229 ended! Reward: -30000\n",
      "Eps reduced to 0.2380\n",
      "Episodes: 229. Steps: 1040000. Score: -26894.96 Alpha: 5.1728e-06 Eps: 0.23798\n",
      "reward =  -19696.0\n",
      "q =  [-6067.476 -6065.849 -6061.952]\n",
      "w norm =  [ 435.72   435.614  435.321]\n",
      "Episode #230 ended! Reward: -30000\n",
      "Eps reduced to 0.2368\n",
      "Episodes: 230. Steps: 1045000. Score: -26894.96 Alpha: 5.1728e-06 Eps: 0.23679\n",
      "reward =  -14729.0\n",
      "q =  [-6083.774 -6083.657 -6073.937]\n",
      "w norm =  [ 436.895  436.897  436.186]\n",
      "Episode #231 ended! Reward: -30000\n",
      "Eps reduced to 0.2356\n",
      "Episodes: 231. Steps: 1050000. Score: -26894.96 Alpha: 5.1728e-06 Eps: 0.23561\n",
      "reward =  -9690.0\n",
      "q =  [-6082.542 -6082.193 -6081.69 ]\n",
      "w norm =  [ 436.819  436.808  436.752]\n",
      "Episode #232 ended! Reward: -37759\n",
      "Eps reduced to 0.2344\n",
      "Episodes: 232. Steps: 1055000. Score: -27047.30 Alpha: 5.1728e-06 Eps: 0.23443\n",
      "reward =  -5328.0\n",
      "q =  [-6103.755 -6103.391 -6104.444]\n",
      "w norm =  [ 438.34   438.326  438.386]\n",
      "Episode #233 ended! Reward: -30000\n",
      "Eps reduced to 0.2333\n",
      "Episodes: 233. Steps: 1060000. Score: -27176.90 Alpha: 5.1728e-06 Eps: 0.23326\n",
      "reward =  -188.0\n",
      "q =  [-6125.431 -6125.296 -6126.317]\n",
      "w norm =  [ 439.892  439.894  439.962]\n",
      "Episodes: 233. Steps: 1065000. Score: -27176.90 Alpha: 5.1728e-06 Eps: 0.23326\n",
      "reward =  -25117.0\n",
      "q =  [-6134.476 -6132.335 -6133.89 ]\n",
      "w norm =  [ 440.542  440.398  440.517]\n",
      "Episode #234 ended! Reward: -30000\n",
      "Eps reduced to 0.2321\n",
      "Episodes: 234. Steps: 1070000. Score: -27301.86 Alpha: 5.1728e-06 Eps: 0.23209\n",
      "reward =  -20137.0\n",
      "q =  [-6154.87  -6154.24  -6154.838]\n",
      "w norm =  [ 442.005  441.966  442.028]\n",
      "Episode #235 ended! Reward: -30000\n",
      "Eps reduced to 0.2309\n",
      "Episode #236 ended! Reward: -12457\n",
      "Eps reduced to 0.2298\n",
      "Episodes: 236. Steps: 1075000. Score: -27236.51 Alpha: 5.1728e-06 Eps: 0.22978\n",
      "reward =  -8820.0\n",
      "q =  [-6172.078 -6170.521 -6171.043]\n",
      "w norm =  [ 443.268  443.138  443.205]\n",
      "Episode #237 ended! Reward: -31913\n",
      "Eps reduced to 0.2286\n",
      "Episode #238 ended! Reward: -16294\n",
      "Eps reduced to 0.2275\n",
      "Episodes: 238. Steps: 1080000. Score: -27118.58 Alpha: 5.1728e-06 Eps: 0.22749\n",
      "reward =  -2581.0\n",
      "q =  [-6183.501 -6179.672 -6180.364]\n",
      "w norm =  [ 444.107  443.795  443.876]\n",
      "Episode #239 ended! Reward: -18726\n",
      "Eps reduced to 0.2263\n",
      "Episode #240 ended! Reward: -15546\n",
      "Eps reduced to 0.2252\n",
      "Episode #241 ended! Reward: -15554\n",
      "Eps reduced to 0.2241\n",
      "Episodes: 241. Steps: 1085000. Score: -26955.88 Alpha: 5.1728e-06 Eps: 0.22409\n",
      "reward =  -1276.0\n",
      "q =  [-6174.954 -6178.207 -6179.82 ]\n",
      "w norm =  [ 443.5    443.699  443.856]\n",
      "Episode #242 ended! Reward: -13764\n",
      "Eps reduced to 0.2230\n",
      "Episodes: 242. Steps: 1090000. Score: -26793.52 Alpha: 5.1728e-06 Eps: 0.22297\n",
      "reward =  -20638.0\n",
      "q =  [-6201.619 -6201.236 -6199.502]\n",
      "w norm =  [ 445.422  445.374  445.271]\n",
      "Episode #243 ended! Reward: -30000\n",
      "Eps reduced to 0.2219\n",
      "Episodes: 243. Steps: 1095000. Score: -26793.52 Alpha: 5.1728e-06 Eps: 0.22186\n",
      "reward =  -15536.0\n",
      "q =  [-6220.487 -6217.699 -6219.16 ]\n",
      "w norm =  [ 446.791  446.575  446.689]\n",
      "Episode #244 ended! Reward: -30000\n",
      "Eps reduced to 0.2207\n",
      "Episodes: 244. Steps: 1100000. Score: -26793.52 Alpha: 5.1728e-06 Eps: 0.22075\n",
      "reward =  -10442.0\n",
      "q =  [-6236.448 -6238.431 -6236.544]\n",
      "w norm =  [ 447.937  448.081  447.939]\n",
      "Episode #245 ended! Reward: -30000\n",
      "Eps reduced to 0.2196\n",
      "Episodes: 245. Steps: 1105000. Score: -26793.52 Alpha: 5.1728e-06 Eps: 0.21964\n",
      "reward =  -5377.0\n",
      "q =  [-6263.12  -6261.385 -6260.483]\n",
      "w norm =  [ 449.861  449.739  449.661]\n",
      "Episode #246 ended! Reward: -38950\n",
      "Eps reduced to 0.2185\n",
      "Episodes: 246. Steps: 1110000. Score: -26883.02 Alpha: 5.1728e-06 Eps: 0.21854\n",
      "reward =  -419.0\n",
      "q =  [-6283.471 -6283.861 -6283.711]\n",
      "w norm =  [ 451.324  451.356  451.331]\n",
      "Episodes: 246. Steps: 1115000. Score: -26883.02 Alpha: 5.1728e-06 Eps: 0.21854\n",
      "reward =  -25535.0\n",
      "q =  [-6303.237 -6299.244 -6302.958]\n",
      "w norm =  [ 452.747  452.461  452.711]\n",
      "Episode #247 ended! Reward: -30000\n",
      "Eps reduced to 0.2175\n",
      "Episodes: 247. Steps: 1120000. Score: -26883.02 Alpha: 5.1728e-06 Eps: 0.21745\n",
      "reward =  -20691.0\n",
      "q =  [-6316.556 -6314.861 -6316.472]\n",
      "w norm =  [ 453.702  453.579  453.687]\n",
      "Episode #248 ended! Reward: -30000\n",
      "Eps reduced to 0.2164\n",
      "Episodes: 248. Steps: 1125000. Score: -27003.78 Alpha: 5.1728e-06 Eps: 0.21636\n",
      "reward =  -15684.0\n",
      "q =  [-6323.137 -6318.596 -6321.783]\n",
      "w norm =  [ 454.176  453.849  454.077]\n",
      "Episode #249 ended! Reward: -30000\n",
      "Eps reduced to 0.2153\n",
      "Episodes: 249. Steps: 1130000. Score: -27111.48 Alpha: 5.1728e-06 Eps: 0.21528\n",
      "reward =  -10509.0\n",
      "q =  [-6340.673 -6340.787 -6340.759]\n",
      "w norm =  [ 455.432  455.44   455.439]\n",
      "Episode #250 ended! Reward: -30000\n",
      "Eps reduced to 0.2142\n",
      "Episodes: 250. Steps: 1135000. Score: -27066.56 Alpha: 5.1728e-06 Eps: 0.21421\n",
      "reward =  -5458.0\n",
      "q =  [-6350.556 -6349.785 -6351.144]\n",
      "w norm =  [ 456.146  456.085  456.19 ]\n",
      "Episode #251 ended! Reward: -30000\n",
      "Eps reduced to 0.2131\n",
      "Episodes: 251. Steps: 1140000. Score: -27237.10 Alpha: 5.1728e-06 Eps: 0.21313\n",
      "reward =  -468.0\n",
      "q =  [-6366.648 -6365.68  -6366.158]\n",
      "w norm =  [ 457.312  457.225  457.265]\n",
      "Episode #252 ended! Reward: -33364\n",
      "Eps reduced to 0.2121\n",
      "Episodes: 252. Steps: 1145000. Score: -27270.74 Alpha: 5.1728e-06 Eps: 0.21207\n",
      "reward =  -774.0\n",
      "q =  [-6365.438 -6374.28  -6373.624]\n",
      "w norm =  [ 457.233  457.844  457.802]\n",
      "Episode #253 ended! Reward: -21936\n",
      "Eps reduced to 0.2110\n",
      "Episodes: 253. Steps: 1150000. Score: -27190.10 Alpha: 5.1728e-06 Eps: 0.21101\n",
      "reward =  -13912.0\n",
      "q =  [-6389.57  -6389.692 -6389.808]\n",
      "w norm =  [ 458.969  458.954  458.965]\n",
      "Episode #254 ended! Reward: -30000\n",
      "Eps reduced to 0.2100\n",
      "Episodes: 254. Steps: 1155000. Score: -27190.10 Alpha: 5.1728e-06 Eps: 0.20995\n",
      "reward =  -8602.0\n",
      "q =  [-6400.073 -6400.256 -6400.09 ]\n",
      "w norm =  [ 459.729  459.715  459.714]\n",
      "Episode #255 ended! Reward: -26538\n",
      "Eps reduced to 0.2089\n",
      "Episodes: 255. Steps: 1160000. Score: -27155.48 Alpha: 5.1728e-06 Eps: 0.2089\n",
      "reward =  -16060.0\n",
      "q =  [-6419.124 -6411.497 -6414.582]\n",
      "w norm =  [ 461.1    460.522  460.762]\n",
      "Episode #256 ended! Reward: -30000\n",
      "Eps reduced to 0.2079\n",
      "Episodes: 256. Steps: 1165000. Score: -27155.48 Alpha: 5.1728e-06 Eps: 0.20786\n",
      "reward =  -11063.0\n",
      "q =  [-6423.546 -6423.846 -6422.608]\n",
      "w norm =  [ 461.426  461.42   461.34 ]\n",
      "Episode #257 ended! Reward: -30000\n",
      "Eps reduced to 0.2068\n",
      "Episodes: 257. Steps: 1170000. Score: -27113.96 Alpha: 5.1728e-06 Eps: 0.20682\n",
      "reward =  -5932.0\n",
      "q =  [-6435.868 -6433.525 -6433.942]\n",
      "w norm =  [ 462.308  462.122  462.154]\n",
      "Episode #258 ended! Reward: -30000\n",
      "Eps reduced to 0.2058\n",
      "Episodes: 258. Steps: 1175000. Score: -27225.46 Alpha: 5.1728e-06 Eps: 0.20579\n",
      "reward =  -1000.0\n",
      "q =  [-6455.639 -6457.208 -6455.714]\n",
      "w norm =  [ 463.723  463.836  463.712]\n",
      "Episode #259 ended! Reward: -30994\n",
      "Eps reduced to 0.2048\n",
      "Episodes: 259. Steps: 1180000. Score: -27177.06 Alpha: 5.1728e-06 Eps: 0.20476\n",
      "reward =  -4134.0\n",
      "q =  [-6473.532 -6474.111 -6474.482]\n",
      "w norm =  [ 465.004  465.055  465.057]\n",
      "Episode #260 ended! Reward: -16184\n",
      "Eps reduced to 0.2037\n",
      "Episode #261 ended! Reward: -16151\n",
      "Eps reduced to 0.2027\n",
      "Episodes: 261. Steps: 1185000. Score: -27146.26 Alpha: 5.1728e-06 Eps: 0.20271\n",
      "reward =  -14143.0\n",
      "q =  [-6492.878 -6491.807 -6493.974]\n",
      "w norm =  [ 466.389  466.327  466.464]\n",
      "Episode #262 ended! Reward: -30000\n",
      "Eps reduced to 0.2017\n",
      "Episodes: 262. Steps: 1190000. Score: -27146.26 Alpha: 5.1728e-06 Eps: 0.2017\n",
      "reward =  -9076.0\n",
      "q =  [-6517.678 -6516.088 -6517.104]\n",
      "w norm =  [ 468.175  468.069  468.171]\n",
      "Episode #263 ended! Reward: -30000\n",
      "Eps reduced to 0.2007\n",
      "Episodes: 263. Steps: 1195000. Score: -27146.26 Alpha: 5.1728e-06 Eps: 0.20069\n",
      "reward =  -3864.0\n",
      "q =  [-6528.425 -6528.856 -6533.596]\n",
      "w norm =  [ 468.947  468.986  469.4  ]\n",
      "Episode #264 ended! Reward: -12830\n",
      "Eps reduced to 0.1997\n",
      "Episodes: 264. Steps: 1200000. Score: -26974.56 Alpha: 5.1728e-06 Eps: 0.19969\n",
      "reward =  -23473.0\n",
      "q =  [-6552.52  -6549.086 -6550.723]\n",
      "w norm =  [ 470.7    470.434  470.665]\n",
      "Episode #265 ended! Reward: -36401\n",
      "Eps reduced to 0.1987\n",
      "Episodes: 265. Steps: 1205000. Score: -27038.57 Alpha: 5.1728e-06 Eps: 0.19869\n",
      "reward =  -20267.0\n",
      "q =  [-6568.057 -6567.221 -6568.232]\n",
      "w norm =  [ 471.855  471.732  471.942]\n",
      "Episode #266 ended! Reward: -30000\n",
      "Eps reduced to 0.1977\n",
      "Episodes: 266. Steps: 1210000. Score: -27038.57 Alpha: 5.1728e-06 Eps: 0.1977\n",
      "reward =  -15160.0\n",
      "q =  [-6580.087 -6580.707 -6580.418]\n",
      "w norm =  [ 472.738  472.702  472.827]\n",
      "Episode #267 ended! Reward: -30000\n",
      "Eps reduced to 0.1967\n",
      "Episodes: 267. Steps: 1215000. Score: -27038.57 Alpha: 5.1728e-06 Eps: 0.19671\n",
      "reward =  -10147.0\n",
      "q =  [-6593.07  -6589.068 -6590.722]\n",
      "w norm =  [ 473.688  473.303  473.569]\n",
      "Episode #268 ended! Reward: -30000\n",
      "Eps reduced to 0.1957\n",
      "Episodes: 268. Steps: 1220000. Score: -27038.57 Alpha: 5.1728e-06 Eps: 0.19573\n",
      "reward =  -5172.0\n",
      "q =  [-6604.524 -6602.436 -6598.932]\n",
      "w norm =  [ 474.513  474.262  474.166]\n",
      "Episode #269 ended! Reward: -30000\n",
      "Eps reduced to 0.1947\n",
      "Episodes: 269. Steps: 1225000. Score: -27030.50 Alpha: 5.1728e-06 Eps: 0.19475\n",
      "reward =  -88.0\n",
      "q =  [-6613.719 -6614.087 -6608.38 ]\n",
      "w norm =  [ 475.179  475.107  474.844]\n",
      "Episodes: 269. Steps: 1230000. Score: -27030.50 Alpha: 5.1728e-06 Eps: 0.19475\n",
      "reward =  -25079.0\n",
      "q =  [-6599.164 -6602.375 -6597.834]\n",
      "w norm =  [ 474.144  474.28   474.089]\n",
      "Episode #270 ended! Reward: -30000\n",
      "Eps reduced to 0.1938\n",
      "Episodes: 270. Steps: 1235000. Score: -27030.50 Alpha: 5.1728e-06 Eps: 0.19377\n",
      "reward =  -20105.0\n",
      "q =  [-6616.899 -6618.208 -6616.816]\n",
      "w norm =  [ 475.415  475.433  475.451]\n",
      "Episode #271 ended! Reward: -30000\n",
      "Eps reduced to 0.1928\n",
      "Episode #272 ended! Reward: -15965\n",
      "Eps reduced to 0.1918\n",
      "Episode #273 ended! Reward: -15521\n",
      "Eps reduced to 0.1909\n",
      "Episodes: 273. Steps: 1240000. Score: -26855.61 Alpha: 5.1728e-06 Eps: 0.19088\n",
      "reward =  -2112.0\n",
      "q =  [-6621.477 -6634.39  -6629.732]\n",
      "w norm =  [ 475.743  476.634  476.375]\n",
      "Episode #274 ended! Reward: -10966\n",
      "Eps reduced to 0.1899\n",
      "Episode #275 ended! Reward: -19503\n",
      "Eps reduced to 0.1890\n",
      "Episode #276 ended! Reward: -19201\n",
      "Eps reduced to 0.1880\n",
      "Episodes: 276. Steps: 1245000. Score: -26628.30 Alpha: 5.1728e-06 Eps: 0.18803\n",
      "reward =  -1764.0\n",
      "q =  [-6637.695 -6642.48  -6639.878]\n",
      "w norm =  [ 476.911  477.271  477.105]\n",
      "Episode #277 ended! Reward: -15189\n",
      "Eps reduced to 0.1871\n",
      "Episode #278 ended! Reward: -8110\n",
      "Eps reduced to 0.1862\n",
      "Episodes: 278. Steps: 1250000. Score: -26347.53 Alpha: 5.1728e-06 Eps: 0.18616\n",
      "reward =  -15336.0\n",
      "q =  [-6661.286 -6663.535 -6660.666]\n",
      "w norm =  [ 478.61   478.839  478.592]\n",
      "Episode #279 ended! Reward: -30000\n",
      "Eps reduced to 0.1852\n",
      "Episodes: 279. Steps: 1255000. Score: -26347.53 Alpha: 5.1728e-06 Eps: 0.18523\n",
      "reward =  -10445.0\n",
      "q =  [-6686.933 -6684.111 -6684.026]\n",
      "w norm =  [ 480.461  480.364  480.267]\n",
      "Episode #280 ended! Reward: -30000\n",
      "Eps reduced to 0.1843\n",
      "Episodes: 280. Steps: 1260000. Score: -26347.53 Alpha: 5.1728e-06 Eps: 0.1843\n",
      "reward =  -5295.0\n",
      "q =  [-6704.914 -6698.822 -6703.786]\n",
      "w norm =  [ 481.77   481.467  481.684]\n",
      "Episode #281 ended! Reward: -30000\n",
      "Eps reduced to 0.1834\n",
      "Episodes: 281. Steps: 1265000. Score: -26347.53 Alpha: 5.1728e-06 Eps: 0.18338\n",
      "reward =  -135.0\n",
      "q =  [-6717.067 -6715.318 -6717.034]\n",
      "w norm =  [ 482.672  482.656  482.642]\n",
      "Episodes: 281. Steps: 1270000. Score: -26347.53 Alpha: 5.1728e-06 Eps: 0.18338\n",
      "reward =  -25043.0\n",
      "q =  [-6720.233 -6718.596 -6719.736]\n",
      "w norm =  [ 482.915  482.891  482.847]\n",
      "Episode #282 ended! Reward: -30000\n",
      "Eps reduced to 0.1825\n",
      "Episodes: 282. Steps: 1275000. Score: -26347.53 Alpha: 5.1728e-06 Eps: 0.18246\n",
      "reward =  -20004.0\n",
      "q =  [-6733.049 -6733.091 -6726.648]\n",
      "w norm =  [ 483.832  483.928  483.366]\n",
      "Episode #283 ended! Reward: -30000\n",
      "Eps reduced to 0.1815\n",
      "Episodes: 283. Steps: 1280000. Score: -26427.46 Alpha: 5.1728e-06 Eps: 0.18155\n",
      "reward =  -14938.0\n",
      "q =  [-6742.691 -6742.806 -6737.383]\n",
      "w norm =  [ 484.522  484.626  484.15 ]\n",
      "Episode #284 ended! Reward: -30000\n",
      "Eps reduced to 0.1806\n",
      "Episodes: 284. Steps: 1285000. Score: -26427.46 Alpha: 5.1728e-06 Eps: 0.18064\n",
      "reward =  -9856.0\n",
      "q =  [-6747.878 -6747.699 -6742.818]\n",
      "w norm =  [ 484.895  484.976  484.555]\n",
      "Episode #285 ended! Reward: -30000\n",
      "Eps reduced to 0.1797\n",
      "Episodes: 285. Steps: 1290000. Score: -26427.46 Alpha: 5.1728e-06 Eps: 0.17974\n",
      "reward =  -4689.0\n",
      "q =  [-6766.283 -6765.697 -6765.927]\n",
      "w norm =  [ 486.228  486.263  486.221]\n",
      "Episodes: 285. Steps: 1295000. Score: -26427.46 Alpha: 5.1728e-06 Eps: 0.17974\n",
      "reward =  -29577.0\n",
      "q =  [-6776.497 -6769.266 -6774.885]\n",
      "w norm =  [ 486.974  486.519  486.865]\n",
      "Episode #286 ended! Reward: -30000\n",
      "Eps reduced to 0.1788\n",
      "Episodes: 286. Steps: 1300000. Score: -26524.43 Alpha: 5.1728e-06 Eps: 0.17884\n",
      "reward =  -24410.0\n",
      "q =  [-6791.685 -6791.111 -6791.101]\n",
      "w norm =  [ 488.09   488.081  488.028]\n",
      "Episode #287 ended! Reward: -30000\n",
      "Eps reduced to 0.1779\n",
      "Episodes: 287. Steps: 1305000. Score: -26524.43 Alpha: 5.1728e-06 Eps: 0.17794\n",
      "reward =  -19252.0\n",
      "q =  [-6800.941 -6799.117 -6801.663]\n",
      "w norm =  [ 488.798  488.657  488.791]\n",
      "Episode #288 ended! Reward: -30000\n",
      "Eps reduced to 0.1771\n",
      "Episodes: 288. Steps: 1310000. Score: -26524.43 Alpha: 5.1728e-06 Eps: 0.17706\n",
      "reward =  -14188.0\n",
      "q =  [-6823.042 -6820.911 -6822.591]\n",
      "w norm =  [ 490.432  490.216  490.312]\n",
      "Episode #289 ended! Reward: -30000\n",
      "Eps reduced to 0.1762\n",
      "Episode #290 ended! Reward: -16343\n",
      "Eps reduced to 0.1753\n",
      "Episodes: 290. Steps: 1315000. Score: -26387.86 Alpha: 5.1728e-06 Eps: 0.17529\n",
      "reward =  -2315.0\n",
      "q =  [-6830.525 -6831.33  -6834.386]\n",
      "w norm =  [ 490.986  490.965  491.174]\n",
      "Episode #291 ended! Reward: -31383\n",
      "Eps reduced to 0.1744\n",
      "Episodes: 291. Steps: 1320000. Score: -26401.69 Alpha: 5.1728e-06 Eps: 0.17441\n",
      "reward =  -5922.0\n",
      "q =  [-6836.548 -6834.879 -6835.208]\n",
      "w norm =  [ 491.428  491.225  491.244]\n",
      "Episode #292 ended! Reward: -30000\n",
      "Eps reduced to 0.1735\n",
      "Episodes: 292. Steps: 1325000. Score: -26401.69 Alpha: 5.1728e-06 Eps: 0.17354\n",
      "reward =  -779.0\n",
      "q =  [-6850.907 -6848.805 -6850.771]\n",
      "w norm =  [ 492.455  492.241  492.373]\n",
      "Episode #293 ended! Reward: -29599\n",
      "Eps reduced to 0.1727\n",
      "Episodes: 293. Steps: 1330000. Score: -26397.68 Alpha: 5.1728e-06 Eps: 0.17267\n",
      "reward =  -5648.0\n",
      "q =  [-6867.182 -6867.75  -6867.708]\n",
      "w norm =  [ 493.619  493.605  493.593]\n",
      "Episode #294 ended! Reward: -14336\n",
      "Eps reduced to 0.1718\n",
      "Episodes: 294. Steps: 1335000. Score: -26346.32 Alpha: 5.1728e-06 Eps: 0.17181\n",
      "reward =  -24756.0\n",
      "q =  [-6883.595 -6881.848 -6883.759]\n",
      "w norm =  [ 494.8    494.641  494.761]\n",
      "Episode #295 ended! Reward: -30000\n",
      "Eps reduced to 0.1710\n",
      "Episode #296 ended! Reward: -21386\n",
      "Eps reduced to 0.1701\n",
      "Episodes: 296. Steps: 1340000. Score: -26417.33 Alpha: 5.1728e-06 Eps: 0.1701\n",
      "reward =  -8232.0\n",
      "q =  [-6903.428 -6898.755 -6905.18 ]\n",
      "w norm =  [ 496.221  495.876  496.323]\n",
      "Episode #297 ended! Reward: -27924\n",
      "Eps reduced to 0.1692\n",
      "Episodes: 297. Steps: 1345000. Score: -26396.57 Alpha: 5.1728e-06 Eps: 0.16925\n",
      "reward =  -13226.0\n",
      "q =  [-6927.174 -6926.482 -6926.531]\n",
      "w norm =  [ 497.928  497.881  497.889]\n",
      "Episode #298 ended! Reward: -22975\n",
      "Eps reduced to 0.1684\n",
      "Episode #299 ended! Reward: -14448\n",
      "Eps reduced to 0.1676\n",
      "Episode #300 ended! Reward: -14021\n",
      "Eps reduced to 0.1667\n",
      "Alpha reduced to 0.0000\n",
      "Episodes: 300. Steps: 1350000. Score: -26204.83 Alpha: 4.3528e-06 Eps: 0.16672\n",
      "reward =  -7681.0\n",
      "q =  [-6951.63  -6952.873 -6957.727]\n",
      "w norm =  [ 499.686  499.794  500.212]\n",
      "Episode #301 ended! Reward: -20780\n",
      "Eps reduced to 0.1659\n",
      "Episode #302 ended! Reward: -15458\n",
      "Eps reduced to 0.1651\n",
      "Episodes: 302. Steps: 1355000. Score: -26054.33 Alpha: 4.3528e-06 Eps: 0.16506\n",
      "reward =  -12411.0\n",
      "q =  [-6963.268 -6961.192 -6967.174]\n",
      "w norm =  [ 500.524  500.402  500.919]\n",
      "Episode #303 ended! Reward: -30000\n",
      "Eps reduced to 0.1642\n",
      "Episodes: 303. Steps: 1360000. Score: -26054.33 Alpha: 4.3528e-06 Eps: 0.16423\n",
      "reward =  -7372.0\n",
      "q =  [-6985.245 -6982.292 -6983.728]\n",
      "w norm =  [ 502.115  501.922  502.152]\n",
      "Episode #304 ended! Reward: -30000\n",
      "Eps reduced to 0.1634\n",
      "Episodes: 304. Steps: 1365000. Score: -26054.33 Alpha: 4.3528e-06 Eps: 0.16341\n",
      "reward =  -2453.0\n",
      "q =  [-7003.624 -7002.952 -7002.761]\n",
      "w norm =  [ 503.446  503.408  503.542]\n",
      "Episode #305 ended! Reward: -35529\n",
      "Eps reduced to 0.1626\n",
      "Episodes: 305. Steps: 1370000. Score: -26176.98 Alpha: 4.3528e-06 Eps: 0.16259\n",
      "reward =  -865.0\n",
      "q =  [-7015.349 -7016.376 -7016.766]\n",
      "w norm =  [ 504.305  504.378  504.557]\n",
      "Episode #306 ended! Reward: -18407\n",
      "Eps reduced to 0.1618\n",
      "Episode #307 ended! Reward: -17547\n",
      "Eps reduced to 0.1610\n",
      "Episodes: 307. Steps: 1375000. Score: -25936.52 Alpha: 4.3528e-06 Eps: 0.16097\n",
      "reward =  -7054.0\n",
      "q =  [-7025.825 -7025.875 -7025.733]\n",
      "w norm =  [ 505.066  505.064  505.198]\n",
      "Episode #308 ended! Reward: -30000\n",
      "Eps reduced to 0.1602\n",
      "Episodes: 308. Steps: 1380000. Score: -25936.52 Alpha: 4.3528e-06 Eps: 0.16017\n",
      "reward =  -1889.0\n",
      "q =  [-7045.511 -7044.418 -7043.961]\n",
      "w norm =  [ 506.495  506.403  506.502]\n",
      "Episodes: 308. Steps: 1385000. Score: -25936.52 Alpha: 4.3528e-06 Eps: 0.16017\n",
      "reward =  -26953.0\n",
      "q =  [-7060.347 -7062.386 -7060.977]\n",
      "w norm =  [ 507.57   507.708  507.719]\n",
      "Episode #309 ended! Reward: -30000\n",
      "Eps reduced to 0.1594\n",
      "Episode #310 ended! Reward: -20691\n",
      "Eps reduced to 0.1586\n",
      "Episodes: 310. Steps: 1390000. Score: -25843.43 Alpha: 4.3528e-06 Eps: 0.15857\n",
      "reward =  -10657.0\n",
      "q =  [-7076.998 -7081.435 -7075.047]\n",
      "w norm =  [ 508.787  509.137  508.725]\n",
      "Episode #311 ended! Reward: -30000\n",
      "Eps reduced to 0.1578\n",
      "Episodes: 311. Steps: 1395000. Score: -25843.43 Alpha: 4.3528e-06 Eps: 0.15778\n",
      "reward =  -5553.0\n",
      "q =  [-7077.433 -7088.752 -7079.541]\n",
      "w norm =  [ 508.837  509.709  509.046]\n",
      "Episode #312 ended! Reward: -30000\n",
      "Eps reduced to 0.1570\n",
      "Episodes: 312. Steps: 1400000. Score: -25843.43 Alpha: 4.3528e-06 Eps: 0.15699\n",
      "reward =  -556.0\n",
      "q =  [-7093.206 -7094.479 -7090.991]\n",
      "w norm =  [ 509.988  510.173  509.865]\n",
      "Episodes: 312. Steps: 1405000. Score: -25843.43 Alpha: 4.3528e-06 Eps: 0.15699\n",
      "reward =  -25628.0\n",
      "q =  [-7086.429 -7089.632 -7080.218]\n",
      "w norm =  [ 509.512  509.863  509.097]\n",
      "Episode #313 ended! Reward: -30000\n",
      "Eps reduced to 0.1562\n",
      "Episodes: 313. Steps: 1410000. Score: -25830.86 Alpha: 4.3528e-06 Eps: 0.1562\n",
      "reward =  -20503.0\n",
      "q =  [-7079.767 -7078.806 -7077.402]\n",
      "w norm =  [ 509.074  509.094  508.902]\n",
      "Episode #314 ended! Reward: -30000\n",
      "Eps reduced to 0.1554\n",
      "Episodes: 314. Steps: 1415000. Score: -25830.86 Alpha: 4.3528e-06 Eps: 0.15542\n",
      "reward =  -15449.0\n",
      "q =  [-7080.293 -7070.688 -7075.734]\n",
      "w norm =  [ 509.161  508.514  508.787]\n",
      "Episode #315 ended! Reward: -30000\n",
      "Eps reduced to 0.1546\n",
      "Episodes: 315. Steps: 1420000. Score: -25830.86 Alpha: 4.3528e-06 Eps: 0.15464\n",
      "reward =  -10304.0\n",
      "q =  [-7086.557 -7085.249 -7083.025]\n",
      "w norm =  [ 509.652  509.555  509.308]\n",
      "Episode #316 ended! Reward: -30000\n",
      "Eps reduced to 0.1539\n",
      "Episodes: 316. Steps: 1425000. Score: -25942.01 Alpha: 4.3528e-06 Eps: 0.15387\n",
      "reward =  -5330.0\n",
      "q =  [-7094.808 -7091.03  -7093.918]\n",
      "w norm =  [ 510.286  509.971  510.1  ]\n",
      "Episode #317 ended! Reward: -30000\n",
      "Eps reduced to 0.1531\n",
      "Episodes: 317. Steps: 1430000. Score: -26072.47 Alpha: 4.3528e-06 Eps: 0.1531\n",
      "reward =  -361.0\n",
      "q =  [-7101.73  -7097.568 -7101.011]\n",
      "w norm =  [ 510.808  510.442  510.611]\n",
      "Episodes: 317. Steps: 1435000. Score: -26072.47 Alpha: 4.3528e-06 Eps: 0.1531\n",
      "reward =  -25242.0\n",
      "q =  [-7113.227 -7110.16  -7113.387]\n",
      "w norm =  [ 511.669  511.345  511.526]\n",
      "Episode #318 ended! Reward: -30000\n",
      "Eps reduced to 0.1523\n",
      "Episodes: 318. Steps: 1440000. Score: -26231.64 Alpha: 4.3528e-06 Eps: 0.15234\n",
      "reward =  -20360.0\n",
      "q =  [-7122.777 -7122.362 -7125.776]\n",
      "w norm =  [ 512.369  512.217  512.455]\n",
      "Episode #319 ended! Reward: -30000\n",
      "Eps reduced to 0.1516\n",
      "Episodes: 319. Steps: 1445000. Score: -26280.57 Alpha: 4.3528e-06 Eps: 0.15157\n",
      "reward =  -15259.0\n",
      "q =  [-7136.92  -7135.669 -7139.852]\n",
      "w norm =  [ 513.393  513.174  513.506]\n",
      "Episode #320 ended! Reward: -30000\n",
      "Eps reduced to 0.1508\n",
      "Episodes: 320. Steps: 1450000. Score: -26356.49 Alpha: 4.3528e-06 Eps: 0.15082\n",
      "reward =  -10236.0\n",
      "q =  [-7148.049 -7147.892 -7149.629]\n",
      "w norm =  [ 514.195  514.048  514.249]\n",
      "Episode #321 ended! Reward: -29855\n",
      "Eps reduced to 0.1501\n",
      "Episodes: 321. Steps: 1455000. Score: -26505.60 Alpha: 4.3528e-06 Eps: 0.15006\n",
      "reward =  -14710.0\n",
      "q =  [-7156.317 -7154.544 -7156.22 ]\n",
      "w norm =  [ 514.79   514.529  514.748]\n",
      "Episode #322 ended! Reward: -30000\n",
      "Eps reduced to 0.1493\n",
      "Episodes: 322. Steps: 1460000. Score: -26504.98 Alpha: 4.3528e-06 Eps: 0.14931\n",
      "reward =  -10001.0\n",
      "q =  [-7173.164 -7172.705 -7172.409]\n",
      "w norm =  [ 516.     515.83   515.906]\n",
      "Episode #323 ended! Reward: -30000\n",
      "Eps reduced to 0.1486\n",
      "Episodes: 323. Steps: 1465000. Score: -26504.98 Alpha: 4.3528e-06 Eps: 0.14856\n",
      "reward =  -5112.0\n",
      "q =  [-7187.332 -7188.001 -7187.441]\n",
      "w norm =  [ 517.015  516.943  516.98 ]\n",
      "Episode #324 ended! Reward: -16080\n",
      "Eps reduced to 0.1478\n",
      "Episodes: 324. Steps: 1470000. Score: -26365.78 Alpha: 4.3528e-06 Eps: 0.14782\n",
      "reward =  -23571.0\n",
      "q =  [-7198.729 -7199.309 -7198.678]\n",
      "w norm =  [ 517.832  517.765  517.783]\n",
      "Episode #325 ended! Reward: -30000\n",
      "Eps reduced to 0.1471\n",
      "Episode #326 ended! Reward: -25701\n",
      "Eps reduced to 0.1463\n",
      "Episodes: 326. Steps: 1475000. Score: -26322.79 Alpha: 4.3528e-06 Eps: 0.14635\n",
      "reward =  -1350.0\n",
      "q =  [-7217.003 -7217.805 -7217.287]\n",
      "w norm =  [ 519.141  519.111  519.116]\n",
      "Episode #327 ended! Reward: -13082\n",
      "Eps reduced to 0.1456\n",
      "Episode #328 ended! Reward: -24150\n",
      "Eps reduced to 0.1449\n",
      "Episodes: 328. Steps: 1480000. Score: -26095.11 Alpha: 4.3528e-06 Eps: 0.14489\n",
      "reward =  -5678.0\n",
      "q =  [-7235.499 -7237.201 -7236.632]\n",
      "w norm =  [ 520.466  520.531  520.507]\n",
      "Episode #329 ended! Reward: -16985\n",
      "Eps reduced to 0.1442\n",
      "Episode #330 ended! Reward: -14200\n",
      "Eps reduced to 0.1434\n",
      "Episodes: 330. Steps: 1485000. Score: -25806.96 Alpha: 4.3528e-06 Eps: 0.14344\n",
      "reward =  -15932.0\n",
      "q =  [-7255.118 -7254.312 -7255.055]\n",
      "w norm =  [ 521.886  521.812  521.827]\n",
      "Episode #331 ended! Reward: -30000\n",
      "Eps reduced to 0.1427\n",
      "Episodes: 331. Steps: 1490000. Score: -25806.96 Alpha: 4.3528e-06 Eps: 0.14273\n",
      "reward =  -11018.0\n",
      "q =  [-7268.262 -7271.145 -7269.883]\n",
      "w norm =  [ 522.84   523.056  522.889]\n",
      "Episode #332 ended! Reward: -20654\n",
      "Eps reduced to 0.1420\n",
      "Episode #333 ended! Reward: -14220\n",
      "Eps reduced to 0.1413\n",
      "Episode #334 ended! Reward: -17526\n",
      "Eps reduced to 0.1406\n",
      "Episode #335 ended! Reward: -15943\n",
      "Eps reduced to 0.1399\n",
      "Episodes: 335. Steps: 1495000. Score: -25212.80 Alpha: 4.3528e-06 Eps: 0.13989\n",
      "reward =  -2571.0\n",
      "q =  [-7289.315 -7291.431 -7290.426]\n",
      "w norm =  [ 524.355  524.57   524.383]\n",
      "Episode #336 ended! Reward: -28780\n",
      "Eps reduced to 0.1392\n",
      "Episodes: 336. Steps: 1500000. Score: -25376.03 Alpha: 4.3528e-06 Eps: 0.13919\n",
      "reward =  -8309.0\n",
      "q =  [-7309.151 -7309.011 -7309.332]\n",
      "w norm =  [ 525.79   525.856  525.75 ]\n",
      "Episode #337 ended! Reward: -30000\n",
      "Eps reduced to 0.1385\n",
      "Episodes: 337. Steps: 1505000. Score: -25356.90 Alpha: 4.3528e-06 Eps: 0.1385\n",
      "reward =  -3208.0\n",
      "q =  [-7326.284 -7326.411 -7323.633]\n",
      "w norm =  [ 527.035  527.102  526.807]\n",
      "Episodes: 337. Steps: 1510000. Score: -25356.90 Alpha: 4.3528e-06 Eps: 0.1385\n",
      "reward =  -28342.0\n",
      "q =  [-7344.213 -7342.701 -7338.5  ]\n",
      "w norm =  [ 528.337  528.269  527.9  ]\n",
      "Episode #338 ended! Reward: -30000\n",
      "Eps reduced to 0.1378\n",
      "Episodes: 338. Steps: 1515000. Score: -25493.96 Alpha: 4.3528e-06 Eps: 0.1378\n",
      "reward =  -23319.0\n",
      "q =  [-7353.512 -7352.655 -7351.907]\n",
      "w norm =  [ 529.012  528.98   528.868]\n",
      "Episode #339 ended! Reward: -30000\n",
      "Eps reduced to 0.1371\n",
      "Episodes: 339. Steps: 1520000. Score: -25606.70 Alpha: 4.3528e-06 Eps: 0.13712\n",
      "reward =  -18180.0\n",
      "q =  [-7369.716 -7369.934 -7370.401]\n",
      "w norm =  [ 530.174  530.215  530.201]\n",
      "Episode #340 ended! Reward: -31309\n",
      "Eps reduced to 0.1364\n",
      "Episodes: 340. Steps: 1525000. Score: -25764.33 Alpha: 4.3528e-06 Eps: 0.13643\n",
      "reward =  -20368.0\n",
      "q =  [-7387.911 -7387.618 -7387.878]\n",
      "w norm =  [ 531.482  531.479  531.463]\n",
      "Episode #341 ended! Reward: -30000\n",
      "Eps reduced to 0.1357\n",
      "Episodes: 341. Steps: 1530000. Score: -25908.79 Alpha: 4.3528e-06 Eps: 0.13575\n",
      "reward =  -15394.0\n",
      "q =  [-7394.489 -7393.811 -7390.135]\n",
      "w norm =  [ 531.963  531.922  531.647]\n",
      "Episode #342 ended! Reward: -30000\n",
      "Eps reduced to 0.1351\n",
      "Episodes: 342. Steps: 1535000. Score: -26071.15 Alpha: 4.3528e-06 Eps: 0.13507\n",
      "reward =  -10435.0\n",
      "q =  [-7409.786 -7409.371 -7409.395]\n",
      "w norm =  [ 533.067  533.034  533.026]\n",
      "Episode #343 ended! Reward: -30000\n",
      "Eps reduced to 0.1344\n",
      "Episodes: 343. Steps: 1540000. Score: -26071.15 Alpha: 4.3528e-06 Eps: 0.13439\n",
      "reward =  -5433.0\n",
      "q =  [-7428.841 -7428.107 -7422.148]\n",
      "w norm =  [ 534.459  534.376  533.947]\n",
      "Episode #344 ended! Reward: -30000\n",
      "Eps reduced to 0.1337\n",
      "Episodes: 344. Steps: 1545000. Score: -26071.15 Alpha: 4.3528e-06 Eps: 0.13372\n",
      "reward =  -453.0\n",
      "q =  [-7442.374 -7440.171 -7435.403]\n",
      "w norm =  [ 535.462  535.238  534.901]\n",
      "Episodes: 344. Steps: 1550000. Score: -26071.15 Alpha: 4.3528e-06 Eps: 0.13372\n",
      "reward =  -25417.0\n",
      "q =  [-7454.438 -7453.692 -7454.33 ]\n",
      "w norm =  [ 536.356  536.211  536.258]\n",
      "Episode #345 ended! Reward: -30000\n",
      "Eps reduced to 0.1331\n",
      "Episode #346 ended! Reward: -20272\n",
      "Eps reduced to 0.1324\n",
      "Episode #347 ended! Reward: -16979\n",
      "Eps reduced to 0.1317\n",
      "Episodes: 347. Steps: 1555000. Score: -25754.16 Alpha: 4.3528e-06 Eps: 0.13173\n",
      "reward =  -1216.0\n",
      "q =  [-7462.595 -7463.175 -7462.523]\n",
      "w norm =  [ 536.951  536.903  536.854]\n",
      "Episode #348 ended! Reward: -24154\n",
      "Eps reduced to 0.1311\n",
      "Episode #349 ended! Reward: -15102\n",
      "Eps reduced to 0.1304\n",
      "Episodes: 349. Steps: 1560000. Score: -25546.72 Alpha: 4.3528e-06 Eps: 0.13041\n",
      "reward =  -5310.0\n",
      "q =  [-7476.801 -7476.756 -7472.063]\n",
      "w norm =  [ 537.97   537.906  537.541]\n",
      "Episode #350 ended! Reward: -27846\n",
      "Eps reduced to 0.1298\n",
      "Episodes: 350. Steps: 1565000. Score: -25525.18 Alpha: 4.3528e-06 Eps: 0.12976\n",
      "reward =  -10539.0\n",
      "q =  [-7485.276 -7486.974 -7485.712]\n",
      "w norm =  [ 538.576  538.672  538.547]\n",
      "Episode #351 ended! Reward: -30000\n",
      "Eps reduced to 0.1291\n",
      "Episodes: 351. Steps: 1570000. Score: -25525.18 Alpha: 4.3528e-06 Eps: 0.12911\n",
      "reward =  -5463.0\n",
      "q =  [-7506.341 -7507.792 -7506.26 ]\n",
      "w norm =  [ 540.081  540.217  540.024]\n",
      "Episode #352 ended! Reward: -30000\n",
      "Eps reduced to 0.1285\n",
      "Episodes: 352. Steps: 1575000. Score: -25491.54 Alpha: 4.3528e-06 Eps: 0.12847\n",
      "reward =  -358.0\n",
      "q =  [-7521.463 -7521.648 -7520.333]\n",
      "w norm =  [ 541.161  541.25   541.038]\n",
      "Episodes: 352. Steps: 1580000. Score: -25491.54 Alpha: 4.3528e-06 Eps: 0.12847\n",
      "reward =  -25312.0\n",
      "q =  [-7535.663 -7536.922 -7534.247]\n",
      "w norm =  [ 542.176  542.359  542.055]\n",
      "Episode #353 ended! Reward: -30000\n",
      "Eps reduced to 0.1278\n",
      "Episodes: 353. Steps: 1585000. Score: -25572.18 Alpha: 4.3528e-06 Eps: 0.12782\n",
      "reward =  -20054.0\n",
      "q =  [-7552.536 -7553.034 -7552.807]\n",
      "w norm =  [ 543.385  543.515  543.392]\n",
      "Episode #354 ended! Reward: -30000\n",
      "Eps reduced to 0.1272\n",
      "Episodes: 354. Steps: 1590000. Score: -25572.18 Alpha: 4.3528e-06 Eps: 0.12718\n",
      "reward =  -14992.0\n",
      "q =  [-7571.899 -7570.634 -7571.61 ]\n",
      "w norm =  [ 544.782  544.772  544.755]\n",
      "Episode #355 ended! Reward: -30000\n",
      "Eps reduced to 0.1265\n",
      "Episodes: 355. Steps: 1595000. Score: -25606.80 Alpha: 4.3528e-06 Eps: 0.12655\n",
      "reward =  -10058.0\n",
      "q =  [-7579.713 -7578.484 -7577.95 ]\n",
      "w norm =  [ 545.346  545.333  545.221]\n",
      "Episode #356 ended! Reward: -30000\n",
      "Eps reduced to 0.1259\n",
      "Episodes: 356. Steps: 1600000. Score: -25606.80 Alpha: 4.3528e-06 Eps: 0.12592\n",
      "reward =  -4847.0\n",
      "q =  [-7587.704 -7587.7   -7583.373]\n",
      "w norm =  [ 545.922  545.991  545.612]\n",
      "Episodes: 356. Steps: 1605000. Score: -25606.80 Alpha: 4.3528e-06 Eps: 0.12592\n",
      "reward =  -29725.0\n",
      "q =  [-7594.958 -7592.595 -7592.385]\n",
      "w norm =  [ 546.457  546.341  546.271]\n",
      "Episode #357 ended! Reward: -30000\n",
      "Eps reduced to 0.1253\n",
      "Episode #358 ended! Reward: -23606\n",
      "Eps reduced to 0.1247\n",
      "Episodes: 358. Steps: 1610000. Score: -25542.86 Alpha: 4.3528e-06 Eps: 0.12466\n",
      "reward =  -9535.0\n",
      "q =  [-7611.129 -7610.1   -7610.247]\n",
      "w norm =  [ 547.626  547.592  547.558]\n",
      "Episode #359 ended! Reward: -30000\n",
      "Eps reduced to 0.1240\n",
      "Episodes: 359. Steps: 1615000. Score: -25532.92 Alpha: 4.3528e-06 Eps: 0.12404\n",
      "reward =  -4471.0\n",
      "q =  [-7626.058 -7624.032 -7624.22 ]\n",
      "w norm =  [ 548.709  548.588  548.578]\n",
      "Episodes: 359. Steps: 1620000. Score: -25532.92 Alpha: 4.3528e-06 Eps: 0.12404\n",
      "reward =  -29437.0\n",
      "q =  [-7641.946 -7639.133 -7636.76 ]\n",
      "w norm =  [ 549.866  549.667  549.48 ]\n",
      "Episode #360 ended! Reward: -30000\n",
      "Eps reduced to 0.1234\n",
      "Episodes: 360. Steps: 1625000. Score: -25671.08 Alpha: 4.3528e-06 Eps: 0.12342\n",
      "reward =  -24485.0\n",
      "q =  [-7653.248 -7651.071 -7651.399]\n",
      "w norm =  [ 550.698  550.52   550.538]\n",
      "Episode #361 ended! Reward: -30000\n",
      "Eps reduced to 0.1228\n",
      "Episodes: 361. Steps: 1630000. Score: -25809.57 Alpha: 4.3528e-06 Eps: 0.1228\n",
      "reward =  -19428.0\n",
      "q =  [-7664.519 -7658.636 -7665.985]\n",
      "w norm =  [ 551.51   551.06   551.602]\n",
      "Episode #362 ended! Reward: -30000\n",
      "Eps reduced to 0.1222\n",
      "Episodes: 362. Steps: 1635000. Score: -25809.57 Alpha: 4.3528e-06 Eps: 0.12218\n",
      "reward =  -14338.0\n",
      "q =  [-7666.505 -7662.445 -7663.828]\n",
      "w norm =  [ 551.652  551.339  551.459]\n",
      "Episode #363 ended! Reward: -30000\n",
      "Eps reduced to 0.1216\n",
      "Episodes: 363. Steps: 1640000. Score: -25809.57 Alpha: 4.3528e-06 Eps: 0.12157\n",
      "reward =  -9274.0\n",
      "q =  [-7670.694 -7671.527 -7667.444]\n",
      "w norm =  [ 551.951  552.004  551.737]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-f987ee6bf316>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mreload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlib\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_transform\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha_init\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/timgaripov/work/temp/RL/skiing/lib.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(env, state_transform, n_frames, num_episodes, alpha_init)\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA_CNT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0mn_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         \u001b[0mn_state_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlast_reps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_transform\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlast_reprs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m         \u001b[0mreplay_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_state_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/timgaripov/work/temp/RL/skiing/lib.py\u001b[0m in \u001b[0;36mprocess_state\u001b[0;34m(state, state_transform, last_reprs)\u001b[0m\n\u001b[1;32m     35\u001b[0m                   \u001b[0mstate_transform\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m                   last_reprs):\n\u001b[0;32m---> 37\u001b[0;31m     \u001b[0ms_repr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m     \u001b[0mlast_reprs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mlast_reprs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms_repr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-f769c9cb36e9>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(state)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mstd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mstate_transform\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0modf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_simple_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrgb2gray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m255\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'uint8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m60\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m60\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mstd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose)\u001b[0m\n\u001b[1;32m   1266\u001b[0m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1267\u001b[0m         return self._predict_loop(f, ins,\n\u001b[0;32m-> 1268\u001b[0;31m                                   batch_size=batch_size, verbose=verbose)\n\u001b[0m\u001b[1;32m   1269\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1270\u001b[0m     def train_on_batch(self, x, y,\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_predict_loop\u001b[0;34m(self, f, ins, batch_size, verbose)\u001b[0m\n\u001b[1;32m    944\u001b[0m                 \u001b[0mins_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mslice_X\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    945\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 946\u001b[0;31m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    947\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    948\u001b[0m                 \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/backend/theano_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    957\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    958\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 959\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    960\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    961\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/theano/compile/function_module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    864\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    865\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 866\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0moutput_subset\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    867\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    868\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "reload(lib)\n",
    "lib.run(env, state_transform, alpha_init=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fc7474bb0f0>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP4AAAD+CAYAAAAalrhRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAADIVJREFUeJzt3UGMnPV9xvHv49JQCI0hxLFLLJhGUR1SVcI9mCQcsqRu\nQhIpkAMVbSOZhBwqpQoSUoShB4R6KQeUROqhh5LUihpCRETsQ1sMMraUVqZEmGLJYKKULbGQF6QC\nEWqLkvjXw75Wx+6OZ7w778yS//cjvdI7/31n3kc788z/nXff3U1VIaktG+YdQNLsWXypQRZfapDF\nlxpk8aUGWXypQWsqfpIbkjyf5IUkd04rlKR+ZbU/x0+yAXgB+APgZeAp4Jaqen568ST1YS0z/g7g\nx1X1H1X1c+C7wI3TiSWpTxes4b7vA346dPsEy28GZ0jipYHSnFRVVhpfS/FXesARJf8YsAgMhpb1\n4CCwMOcMZzuImSZxkPWXCeaba7FbTjs0csu1FP8EcOXQ7a0sf9ZfwQLr94mSflUMOHNSHV38tXzG\nfwr4QJKrkrwDuAXYt4bHkzQjq57xq+qXSf4c2M/yG8gDVfXc6HsMVrurHg3mHWAFg3kHWMFg3gFW\nMJh3gBEG8w4wkVX/OG/iHSQF9/S6D0kruXfkyT2v3JMaZPGlBll8qUEWX2qQxZcaZPGlBll8qUEW\nX2qQxZcaZPGlBll8qUEWX2qQxZcaZPGlBll8qUEWX2qQxZcaZPGlBll8qUEWX2qQxZcaZPGlBll8\nqUEWX2qQxZcaZPGlBll8qUEWX2rQ2OIneSDJUpJnh8YuS7I/yfEkjybZ2G9MSdM0yYz/LeCTZ43t\nBh6vqm3AAeCuaQeT1J+xxa+qHwKvnTV8I7CnW98D3DTlXJJ6tNrP+O+tqiWAqjoJbJpeJEl98+Se\n1KALVnm/pSSbq2opyRbglXNvfnBofdAtkqZrsVvGm7T46ZbT9gG3AvcBu4C95777woS7kbR6A86c\nVA+N3HKSH+d9B/gX4HeSvJTkC8BfAX+Y5Diws7st6W1i7IxfVX8y4ks7p5xF0ox4ck9qkMWXGmTx\npQZZfKlBFl9qkMWXGmTxpQZZfKlBq71WX29jf1fPTeVxbs3VU3kczZ4zvtQgiy81yOJLDbL4UoMs\nvtQgiy81yOJLDbL4UoMsvtQgiy81yOJLDbL4UoMsvtQgiy81yOJLDbL4UoMsvtQgiy81yOJLDbL4\nUoMm+TfZW5McSHIsydEkX+nGL0uyP8nxJI8m2dh/XEnTMMmM/wvgjqr6EPAR4MtJPgjsBh6vqm3A\nAeCu/mJKmqaxxa+qk1X1TLf+JvAcsBW4EdjTbbYHuKmvkJKm67w+4ycZANcAh4HNVbUEy28OwKZp\nh5PUj4n/oUaSS4CHgdur6s0kNfluDg6tD7pF0nQtdst4ExU/yQUsl/7bVbW3G15KsrmqlpJsAV4Z\n/QgLE4XRbPgfcH5VDThzUj00cstJD/W/CRyrqm8Mje0Dbu3WdwF7z76TpPVp7Iyf5DrgT4GjSY4A\nBdwN3Ad8L8kXgZeAm/sMKml6xha/qv4Z+LURX9453TiSZsEr96QGWXypQRZfapDFlxpk8aUGWXyp\nQRZfapDFlxpk8aUGWXypQRZfapDFlxpk8aUGWXypQRZfapDFlxpk8aUGWXypQRZfapDFlxpk8aUG\nWXypQRZfapDFlxpk8aUGWXypQRZfapDFlxo0tvhJLkzyZJIjSY4muacbHyQ5nOR4kgeTjP0HnJLW\nh7HFr6q3gOurajtwDfCpJNey/G+y76+qbcDrwG29JpU0NRMd6lfVf3WrF7L8r7ULuB74fje+B/jc\n1NNJ6sVExU+yIckR4CTwGPAT4PWqOtVtcgK4op+IkqZtos/lXcG3J3kX8Ahw9UqbjX6Eg0Prg26R\nNF2L3TLeeZ2Qq6qfJTkEfBi4NMmG7k1hK/Dy6HsunM9uJK3KgDMn1UMjt5zkrP57kmzs1i8CdgLH\ngCeAm7vNdgF7V5VV0sxNMuP/FrAnyQaW3ygeqqp/SPIc8N0kfwkcAR7oMaekKRpb/Ko6Cvz+CuMv\nAtf2EUpSv7xyT2qQxZcaZPGlBll8qUH+Yo20Dvz3m/dOvO1Fl9yz5v0540sNsvhSgyy+1CCLLzXI\n4ksNsvhSgyy+1CCLLzXI4ksNsvhSgyy+1CCLLzXI4ksNsvhSg/y1XGkdmMav2p4PZ3ypQRZfapDF\nlxrkZ/x15B4m//NLs3Ivs/3sqdlwxpcaZPGlBll8qUEWX2rQxMVPsiHJ00n2dbcHSQ4nOZ7kwSSe\nKJTeJs5nxr8dODZ0+z7g/qraBrwO3DbNYJL6M1Hxk2wFPg387dDwx4Hvd+t7gM9NN5qkvkw6438N\n+CpQAEkuB16rqlPd108AV0w/nqQ+jP1cnuQzwFJVPZNk4fRwtwyr0Y9ycGh90C2SpmuxW8ab5ITc\ndcBnk3wauAj4TeDrwMYkG7pZfyvw8uiHWJgojKS1GHDmpHpo5JZjD/Wr6u6qurKq3g/cAhyoqs8D\nTwA3d5vtAvauMq2kGVvLz/F3A3ckeQF4N/DAdCJJ6tt5/ey9qg7RHT9U1YvAtX2EktQvr9yTGuTV\ndudp4//82arv+8Zv/M05v97Xr8CuJTNjMuvtyRlfapDFlxpk8aUGWXypQRZfapDFlxpk8aUGWXyp\nQRZfapDFlxpk8aUGWXypQRZfapDFlxpk8aUGWXypQRZfapDFlxpk8aUGWXypQRZfapB/Zfc8jftL\nuevR2zGz+uWMLzXI4ksNsvhSgyy+1KCJTu4lWQTeAE4BP6+qHUkuAx4CrgIWgT+qqjd6yilpiiad\n8U8BC1W1vap2dGO7gcerahtwALirj4CSpm/S4meFbW8E9nTre4CbphVKUr8mLX4BjyZ5KsmXurHN\nVbUEUFUngU19BJQ0fZNewPPRqjqZZBOwP8lxlt8MJnRwaH3QLZKma7Fbxpuo+N2MTlW9muQHwA5g\nKcnmqlpKsgV4ZfQjLEwURtJaDDhzUj00csuxh/pJLk5ySbf+TuATwFFgH3Brt9kuYO9qokqavUlm\n/M3AI0mq2/7vq2p/kh8B30vyReAl4OYec0qaorHFr6oXgWtWGP9PYGcfoST1yyv3pAZZfKlBFl9q\nkMWXGmTxpQZZfKlBFl9qkMWXGmTxpQZZfKlBFl9qkMWXGmTxpQZZfKlBFl9qkMWXGmTxpQZZfKlB\nFl9qkMWXGmTxpQZZfKlBFl9qkMWXGmTxpQZZfKlBFl9q0AyLvzi7XU1scd4BVrA47wArWJx3gBUs\nzjvACIvzDjARi7/uLM47wAoW5x1gBYvzDjDC4rwDTMRDfalBFl9qUKqq3x0k/e5A0khVlZXGey++\npPXHQ32pQRZfapDFlxrUe/GT3JDk+SQvJLmz7/2dI8cDSZaSPDs0dlmS/UmOJ3k0ycYZZ9qa5ECS\nY0mOJvnKvHMluTDJk0mOdJnu6cYHSQ53mR5McsGsMg1l25Dk6ST71kOmJItJ/q37Xv1rNzbX19Sk\nei1+kg3AXwOfBH4X+OMkH+xzn+fwrS7HsN3A41W1DTgA3DXjTL8A7qiqDwEfAb7cfX/mlquq3gKu\nr6rtwDXAp5JcC9wH3N9leh24bVaZhtwOHBu6Pe9Mp4CFqtpeVTu6sXm/piZTVb0twIeBfxy6vRu4\ns899jslzFfDs0O3ngc3d+hbg+Xll6zL8ANi5XnIBFwM/AnYArwAbhp7Xf5pxlq3AY8ACsK8be3XO\nmV4ELj9rbF08d+OWvg/13wf8dOj2iW5svXhvVS0BVNVJYNO8giQZsDzDHmb5hTO3XN0h9RHgJMtl\n+wnwelWd6jY5AVwxy0zA14CvAtVlvBx4bc6ZCng0yVNJvtSNzfW5m1Tfn4lWunjACwfOkuQS4GHg\n9qp6c94XPXVl2p7kXcAjwNUrbTarPEk+AyxV1TNJFk4P8/9fX7P+vn20qk4m2QTsT3J8DhlWpe8Z\n/wRw5dDtrcDLPe/zfCwl2QyQZAvLh7Mz1Z2Qehj4dlXtXS+5AKrqZ8Ahlg+jL+3O2cDsn8frgM8m\n+XfgQeDjwNeBjXPMdHpGp6peZflj2g7WyXM3Tt/Ffwr4QJKrkrwDuAXY1/M+z+XsWWIfcGu3vgvY\ne/YdZuCbwLGq+sbQ2NxyJXnP6TPRSS5i+ZzDMeAJ4OZ5ZKqqu6vqyqp6P8uvoQNV9fl5ZkpycXek\nRpJ3Ap8AjrI+XlPjzeAEyA3AceDHwO55ncwAvsPyjPAW8BLwBeAy4PEu32PApTPOdB3wS+AZ4Ajw\ndPf9eve8cgG/1+V4BngW+Itu/LeBJ4EXgIeAX5/T8/gx/u/k3twydfs+/bwdPf3anudzdz6L1+pL\nDfLKPalBFl9qkMWXGmTxpQZZfKlBFl9qkMWXGvS/fNC+IX0TxvYAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fc7475589b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (1000,64) (1000,1,60,60) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-882f560241f4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredicted\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtest_observations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mtest_observations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (1000,64) (1000,1,60,60) "
     ]
    }
   ],
   "source": [
    "np.linalg.norm(predicted - test_observations)**2 / test_observations.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f611b308780>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP4AAAD+CAYAAAAalrhRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAADOxJREFUeJzt3V+MXOV9xvHv49IgCA04xDEQC7ZRVEOqqrgXJoQLTEoD\nSaSQXFDRNpJJiHKTKkRIEYZeGNSbcoECUq+qksiKGkJEROyLFgwyttQ/pkQYsGRsopQtsYgXpEKi\nqBJK4l8v9rgdm13veHfOnKXv9yMd6cy7Z+Y89swz78yZs7OpKiS1Zc3QASRNn8WXGmTxpQZZfKlB\nFl9qkMWXGrSi4ie5McnhJC8nuXNSoST1K8v9HD/JGuBl4I+B14BngVuq6vDk4knqw0pm/M3Aj6vq\nP6vqV8D3gJsmE0tSn85awXU/BPx05PJR5p8MTpLEUwOlgVRVFhpfSfEXusFFSn4tMAvMjCyrwV5g\ny8AZTrUXM41jL6svEwyba7ZbTti36JYrKf5R4NKRyxuYf6+/gC2s3jtK+v9ihpMn1cWLv5L3+M8C\nH0lyWZL3ALcAu1Zwe5KmZNkzflX9JslfAruZfwJ5qKpeWvwaM8vdVY9mhg6wgJmhAyxgZugAC5gZ\nOsAiZoYOMJZlf5w39g6Sgu297kPSQu5d9OCeZ+5JDbL4UoMsvtQgiy81yOJLDbL4UoMsvtQgiy81\nyOJLDbL4UoMsvtQgiy81yOJLDbL4UoMsvtQgiy81yOJLDbL4UoMsvtQgiy81yOJLDbL4UoMsvtQg\niy81yOJLDbL4UoMsvtQgiy81aMniJ3koyVySF0fG1ibZneRIkieSnN9vTEmTNM6M/23ghlPGtgFP\nVdVGYA9w16SDSerPksWvqn8G3jxl+CZgR7e+A/jchHNJ6tFy3+N/sKrmAKrqGLBucpEk9c2De1KD\nzlrm9eaSrK+quSQXAa+ffvO9I+sz3SJpsma7ZWnjFj/dcsIu4FbgPmArsPP0V9+y4Oi/1Q/H3P10\nXR0PWejdaIaTJ9V9i245zsd53wX+Ffi9JK8m+SLwN8CfJDkCXN9dlvQuseSMX1V/vsiPrp9wFklT\n4sE9qUEWX2qQxZcaZPGlBll8qUEWX2qQxZcaZPGlBll8qUEWX2qQxZcaZPGlBll8qUEWX2qQxZca\nZPGlBll8qUEWX2qQxZcatNyv11YP6vJ7h47wDjm8fegI6oEzvtQgiy81yOJLDRr0Pb5/sUYahjO+\n1CCLLzXI4ksN8nP8AT1QPzvp8oN8Zezr3n7F3006zlhOzXwmvp6LJ5hEK+GMLzVonD+TvSHJniSH\nkhxM8rVufG2S3UmOJHkiyfn9x5U0CePM+L8G7qiqjwJXA19NcjmwDXiqqjYCe4C7+ospaZKWfI9f\nVceAY936L5O8BGwAbgKu7TbbAexl/slAU+A59FqJM3qPn2QGuBLYD6yvqjn43yeHdZMOJ6kfYx/V\nT3Ie8Chwezfz1/i72TuyPtMtkiZrtluWNlbxk5zFfOm/U1U7u+G5JOurai7JRcDri9/ClrHCSFqJ\nGU6eVPctuuW4L/W/BRyqqgdHxnYBt3brW4Gdp15J0uq05Iyf5BrgL4CDSQ4ABdwN3Ad8P8mXgFeB\nm/sMKmlyxjmq/y/Aby3y4+snG0fSNHjmntQgiy81yOJLDbL4UoP8tVxp1OP3DJ3gnW68Z+I36Ywv\nNcjiSw2y+FKDLL7UIIsvNcjiSw3y47wBvRu/dfbdmFnv5IwvNcjiSw2y+FKDLL7UIIsvNcjiSw2y\n+FKDLL7UIIsvNcjiSw2y+FKDLL7UIIsvNcjiSw2y+FKDLL7UoCWLn+TsJM8kOZDkYJLt3fhMkv1J\njiR5OIlf6iG9SyxZ/Kp6G7iuqjYBVwKfSnIV838m+/6q2gi8BdzWa1JJEzPWS/2q+u9u9Wzmv66r\ngOuAH3TjO4DPTzydpF6MVfwka5IcAI4BTwI/Ad6qquPdJkeBS/qJKGnSxnpf3hV8U5L3AY8BVyy0\n2eK3sHdkfaZbJE3WbLcs7YwOyFXVL5LsAz4GXJBkTfeksAF4bfFrbjmT3UhalhlOnlT3LbrlOEf1\nP5Dk/G79HOB64BDwNHBzt9lWYOeyskqaunFm/IuBHUnWMP9E8UhV/WOSl4DvJflr4ADwUI85JU3Q\nksWvqoPAHy0w/gpwVR+hJPXLk26kAfzhDfvH37huPO2PX8jjZ7x/T9mVGmTxpQZZfKlBFl9qkMWX\nGmTxpQZZfKlBFl9qkMWXGmTxpQZZfKlBnqsvjbrxnunsZ4nz7/vmjC81yOJLDbL4UoMsvtQgiy81\nyOJLDbL4UoMsvtQgiy81yOJLDbL4UoMsvtQgiy81yOJLDbL4UoPGLn6SNUmeS7KruzyTZH+SI0ke\nTuLv9kvvEmcy498OHBq5fB9wf1VtBN4CbptkMEn9Gav4STYAnwb+fmT4E8APuvUdwOcnG01SX8ad\n8b8JfAMogCQXAm9W1fHu50eBSyYfT1IflnxfnuQzwFxVPZ9ky4nhbhlVi9/K3pH1mW6RNFmz3bK0\ncQ7IXQN8NsmngXOA3wEeAM5Psqab9TcAry1+E1vGCiNpJWY4eVLdt+iWS77Ur6q7q+rSqvowcAuw\np6q+ADwN3NxtthXYucy0kqZsJZ/jbwPuSPIy8H7goclEktS3M/rsvar20b1+qKpXgKv6CCWpX565\nJzXIs+20bNu5d+gIC7qX7UNHWPWc8aUGWXypQRZfapDFlxpk8aUGWXypQX6cJx6ony3rem+e+mta\nU7K2vnLanz/A4v+er+fiScdZlhfy+KD7d8aXGmTxpQZZfKlBvsfXsg11auzp3sNrPM74UoMsvtQg\niy81yOJLDbL4UoMsvtQgiy81yOJLDbL4UoMsvtQgiy81yOJLDbL4UoMsvtSgVJ3mz9pPYgdJ4V82\nkQZwL1W14BekjfX7+ElmgZ8Dx4FfVdXmJGuBR4DLgFngT6vq5xPJK6lX477UPw5sqapNVbW5G9sG\nPFVVG4E9wF19BJQ0eeMWPwtsexOwo1vfAXxuUqEk9Wvc4hfwRJJnk3y5G1tfVXMAVXUMWNdHQEmT\nN+537n28qo4lWQfsTnKE+SeDMe0dWZ/pFkmTNdstSxur+N2MTlW9keSHwGZgLsn6qppLchHw+uK3\nsGWsMJJWYoaTJ9V9i2655Ev9JOcmOa9bfy/wSeAgsAu4tdtsK7BzOVElTd84M/564LH5z+M5C/iH\nqtqd5EfA95N8CXgVuLnHnJImaMniV9UrwJULjP8XcH0foST1y1N2pQZZfKlBFl9qkMWXGmTxpQZZ\nfKlBFl9qkMWXGmTxpQZZfKlBFl9qkMWXGmTxpQZZfKlBFl9qkMWXGmTxpQZZfKlBFl9qkMWXGmTx\npQZZfKlBFl9qkMWXGmTxpQZZfKlBFl9q0BSLPzu9XY1tdugAC5gdOsACZocOsIDZoQMsYnboAGOx\n+KvO7NABFjA7dIAFzA4dYBGzQwcYiy/1pQZZfKlBqap+d5D0uwNJi6qqLDTee/ElrT6+1JcaZPGl\nBll8qUG9Fz/JjUkOJ3k5yZ197+80OR5KMpfkxZGxtUl2JzmS5Ikk508504Yke5IcSnIwydeGzpXk\n7CTPJDnQZdrejc8k2d9lejjJWdPKNJJtTZLnkuxaDZmSzCZ5ofu/+vdubNDH1Lh6LX6SNcDfAjcA\nvw/8WZLL+9znaXy7yzFqG/BUVW0E9gB3TTnTr4E7quqjwNXAV7v/n8FyVdXbwHVVtQm4EvhUkquA\n+4D7u0xvAbdNK9OI24FDI5eHznQc2FJVm6pqczc29GNqPFXV2wJ8DPinkcvbgDv73OcSeS4DXhy5\nfBhY361fBBweKluX4YfA9aslF3Au8CNgM/A6sGbkfn18ylk2AE8CW4Bd3dgbA2d6BbjwlLFVcd8t\ntfT9Uv9DwE9HLh/txlaLD1bVHEBVHQPWDRUkyQzzM+x+5h84g+XqXlIfAI4xX7afAG9V1fFuk6PA\nJdPMBHwT+AZQXcYLgTcHzlTAE0meTfLlbmzQ+25cfb8nWujkAU8cOEWS84BHgdur6pdDn/TUlWlT\nkvcBjwFXLLTZtPIk+QwwV1XPJ9lyYph3Pr6m/f/28ao6lmQdsDvJkQEyLEvfM/5R4NKRyxuA13re\n55mYS7IeIMlFzL+cnarugNSjwHeqaudqyQVQVb8A9jH/MvqC7pgNTP9+vAb4bJL/AB4GPgE8AJw/\nYKYTMzpV9Qbzb9M2s0ruu6X0XfxngY8kuSzJe4BbgF097/N0Tp0ldgG3dutbgZ2nXmEKvgUcqqoH\nR8YGy5XkAyeORCc5h/ljDoeAp4Gbh8hUVXdX1aVV9WHmH0N7quoLQ2ZKcm73So0k7wU+CRxkdTym\nljaFAyA3AkeAHwPbhjqYAXyX+RnhbeBV4IvAWuCpLt+TwAVTznQN8BvgeeAA8Fz3//X+oXIBf9Dl\neB54Efirbvx3gWeAl4FHgN8e6H68lv87uDdYpm7fJ+63gyce20Ped2eyeK6+1CDP3JMaZPGlBll8\nqUEWX2qQxZcaZPGlBll8qUH/A2ca1w1uL7vsAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f61178d6198>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(test_observations[25][0], interpolation='Nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Invalid dimensions for image data",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-364139dd4887>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_observations\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterpolation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Nearest'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mimshow\u001b[0;34m(X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, shape, filternorm, filterrad, imlim, resample, url, hold, data, **kwargs)\u001b[0m\n\u001b[1;32m   3027\u001b[0m                         \u001b[0mfilternorm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfilternorm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilterrad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfilterrad\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3028\u001b[0m                         \u001b[0mimlim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimlim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3029\u001b[0;31m                         **kwargs)\n\u001b[0m\u001b[1;32m   3030\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3031\u001b[0m         \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwashold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/matplotlib/__init__.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(ax, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1817\u001b[0m                     warnings.warn(msg % (label_namer, func.__name__),\n\u001b[1;32m   1818\u001b[0m                                   RuntimeWarning, stacklevel=2)\n\u001b[0;32m-> 1819\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1820\u001b[0m         \u001b[0mpre_doc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1821\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpre_doc\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mimshow\u001b[0;34m(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, shape, filternorm, filterrad, imlim, resample, url, **kwargs)\u001b[0m\n\u001b[1;32m   4920\u001b[0m                               resample=resample, **kwargs)\n\u001b[1;32m   4921\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4922\u001b[0;31m         \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4923\u001b[0m         \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_alpha\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4924\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_clip_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/matplotlib/image.py\u001b[0m in \u001b[0;36mset_data\u001b[0;34m(self, A)\u001b[0m\n\u001b[1;32m    451\u001b[0m         if (self._A.ndim not in (2, 3) or\n\u001b[1;32m    452\u001b[0m                 (self._A.ndim == 3 and self._A.shape[-1] not in (3, 4))):\n\u001b[0;32m--> 453\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid dimensions for image data\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    454\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_imcache\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Invalid dimensions for image data"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQkAAAEACAYAAACgZ4OsAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f61176b5940>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(model.predict(np.array([test_observations[25]]))[0, 0], interpolation='Nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
