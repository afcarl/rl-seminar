{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "import gym\n",
    "import time\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.kernel_approximation import RBFSampler\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "seed = 417"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Action-Value function based Actor-Critic ([description](http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching_files/pg.pdf#25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, env, alpha=0.01, beta=0.01):\n",
    "        self.nA = env.action_space.n\n",
    "        n_components = 100\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        observation_examples = np.array([np.concatenate(([env.action_space.sample()], \n",
    "                                                         env.observation_space.sample())) for x in range(100000)])\n",
    "\n",
    "        self.scaler = StandardScaler()\n",
    "        self.feature_map = FeatureUnion([(\"rbf1\", RBFSampler(n_components=n_components, gamma=1., random_state=1)),\n",
    "                                         (\"rbf01\", RBFSampler(n_components=n_components, gamma=0.1, random_state=1)),\n",
    "                                         (\"rbf10\", RBFSampler(n_components=n_components, gamma=10, random_state=1))])\n",
    "\n",
    "        self.feature_map.fit(self.scaler.fit_transform(observation_examples))\n",
    "        \n",
    "        self.theta = np.random.rand(3 * n_components)\n",
    "        self.w = np.random.rand(3 * n_components)\n",
    "        \n",
    "    def compute_features(self, s, a):\n",
    "        seed = 417\n",
    "        np.random.seed(seed)\n",
    "        return self.feature_map.transform(self.scaler.transform(np.hstack((a, s))[np.newaxis, :]))\n",
    "    \n",
    "    def act(self, s):\n",
    "        self.Phi_s = np.vstack([self.compute_features(s, a) for a in range(self.nA)])\n",
    "        probs = np.exp(np.dot(self.Phi_s, self.theta))\n",
    "        self.probs = probs / np.sum(probs)\n",
    "        return np.random.choice(self.nA, p=self.probs)\n",
    "    \n",
    "    def update(self, s, a, r, sp, ap):\n",
    "        gamma = 0.95\n",
    "        phi = self.Phi_s[a]\n",
    "        Q_old = np.inner(phi, self.w)\n",
    "        Q_new = np.inner(self.compute_features(sp, ap).ravel(), self.w)\n",
    "        \n",
    "        self.theta += self.alpha * (phi - np.sum(self.probs[:, np.newaxis] * self.Phi_s, axis=0)) * Q_old\n",
    "        delta = r + gamma * Q_new - Q_old\n",
    "        self.w += self.beta * delta * phi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "done = False\n",
    "render = True "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gym.envs.registration:Making new env: MountainCar-v0\n",
      "[2017-01-10 11:53:05,132] Making new env: MountainCar-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 0 finished in 500 steps\n",
      "episode 1 finished in 500 steps\n",
      "episode 2 finished in 500 steps\n",
      "episode 3 finished in 500 steps\n",
      "episode 4 finished in 500 steps\n",
      "episode 5 finished in 500 steps\n",
      "episode 6 finished in 500 steps\n",
      "episode 7 finished in 500 steps\n",
      "episode 8 finished in 500 steps\n",
      "episode 9 finished in 500 steps\n",
      "episode 10 finished in 500 steps\n",
      "episode 11 finished in 500 steps\n",
      "episode 12 finished in 500 steps\n",
      "episode 13 finished in 500 steps\n",
      "episode 14 finished in 500 steps\n",
      "episode 15 finished in 500 steps\n",
      "episode 16 finished in 500 steps\n",
      "episode 17 finished in 500 steps\n",
      "episode 18 finished in 500 steps\n",
      "episode 19 finished in 500 steps\n"
     ]
    }
   ],
   "source": [
    "env_name = 'MountainCar-v0'\n",
    "env = gym.make(env_name)\n",
    "np.random.seed(seed)\n",
    "agent = Agent(env, 0.001, 0.1)\n",
    "\n",
    "for e in range(20):\n",
    "    s = env.reset()\n",
    "    t = 0\n",
    "    done = False\n",
    "    a = agent.act(s)\n",
    "    \n",
    "    while not done and t < 500:\n",
    "        if render: env.render()\n",
    "        \n",
    "        sp, r, done, _ = env.step(a)\n",
    "        ap = agent.act(sp)\n",
    "        \n",
    "        agent.update(s, a, r, sp, ap)\n",
    "        \n",
    "        s = sp\n",
    "        t += 1\n",
    "    \n",
    "    print('episode {} finished in {} steps'.format(e, t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Value function based Actor-Critic (can be found in book on p.294)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gamma = 0.95\n",
    "done = False\n",
    "render = True \n",
    "n_components = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gym.envs.registration:Making new env: MountainCar-v0\n",
      "[2017-01-10 11:53:38,439] Making new env: MountainCar-v0\n"
     ]
    }
   ],
   "source": [
    "class FeatureMaker:\n",
    "    def __init__(self, env, n_components=100):\n",
    "        observation_examples = np.array([env.observation_space.sample() for x in range(100000)])\n",
    "\n",
    "        self.scaler = StandardScaler()\n",
    "        self.feature_map = FeatureUnion([(\"rbf1\", RBFSampler(n_components=n_components, gamma=1., random_state=seed)),\n",
    "                                         (\"rbf01\", RBFSampler(n_components=n_components, gamma=0.1, random_state=seed)),\n",
    "                                         (\"rbf10\", RBFSampler(n_components=n_components, gamma=10, random_state=seed))])\n",
    "\n",
    "        self.feature_map.fit(self.scaler.fit_transform(observation_examples))\n",
    "        \n",
    "    def compute_features(self, s):\n",
    "        return self.feature_map.transform(self.scaler.transform(s[np.newaxis, :]))[0]\n",
    "    \n",
    "    \n",
    "env_name = 'MountainCar-v0'\n",
    "env = gym.make(env_name)\n",
    "fm = FeatureMaker(env, n_components=n_components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class PolicyApproximator:\n",
    "    def __init__(self, env, n_components=100, alpha=0.01):\n",
    "        self.alpha = alpha\n",
    "        self.nA = env.action_space.n\n",
    "        self.theta = np.random.rand(3*n_components+1)\n",
    "        \n",
    "    def act(self, s):\n",
    "        self.Phi_s = np.hstack((np.arange(self.nA)[:, np.newaxis], \n",
    "                                np.repeat(fm.compute_features(s)[np.newaxis, :], self.nA, axis=0)))\n",
    "        probs = np.exp(np.dot(self.Phi_s, self.theta))\n",
    "        self.probs = probs / np.sum(probs)\n",
    "        return np.random.choice(self.nA, p=self.probs)\n",
    "    \n",
    "    def update(self, delta, a):\n",
    "        phi = self.Phi_s[a]\n",
    "        self.theta += self.alpha * delta * (phi - np.sum(self.probs[:, np.newaxis] * self.Phi_s, axis=0))\n",
    "        \n",
    "    \n",
    "        \n",
    "class ValueApproximator:\n",
    "    def __init__(self, env, n_components=100, beta=0.01):\n",
    "        self.beta = beta\n",
    "        self.w = np.random.rand(3*n_components)\n",
    "        \n",
    "    def predict(self, s):\n",
    "        return np.inner(self.w, fm.compute_features(s))\n",
    "        \n",
    "    def update(self, delta, s):\n",
    "        self.w += self.beta * delta * fm.compute_features(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 0 finished in 500 steps\n",
      "episode 1 finished in 500 steps\n",
      "episode 2 finished in 500 steps\n",
      "episode 3 finished in 500 steps\n",
      "episode 4 finished in 500 steps\n",
      "episode 5 finished in 500 steps\n",
      "episode 6 finished in 500 steps\n",
      "episode 7 finished in 500 steps\n",
      "episode 8 finished in 500 steps\n",
      "episode 9 finished in 500 steps\n",
      "episode 10 finished in 500 steps\n",
      "episode 11 finished in 500 steps\n",
      "episode 12 finished in 500 steps\n",
      "episode 13 finished in 500 steps\n",
      "episode 14 finished in 500 steps\n",
      "episode 15 finished in 500 steps\n",
      "episode 16 finished in 500 steps\n",
      "episode 17 finished in 500 steps\n",
      "episode 18 finished in 500 steps\n",
      "episode 19 finished in 500 steps\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(seed)\n",
    "policy = PolicyApproximator(env, alpha=0.001)\n",
    "value = ValueApproximator(env, beta=0.1)\n",
    "\n",
    "for e in range(20):\n",
    "    s = env.reset()\n",
    "    t = 0\n",
    "    done = False\n",
    "    \n",
    "    while not done and t < 500:\n",
    "        if render: env.render()\n",
    "        \n",
    "        a = policy.act(s)\n",
    "        sn, r, done, _ = env.step(a)\n",
    "        \n",
    "        V_old = value.predict(s)\n",
    "        V_new = value.predict(sn)\n",
    "        delta = r + gamma * V_new - V_old\n",
    "        \n",
    "        value.update(delta, s)\n",
    "        policy.update(delta, a)\n",
    "        \n",
    "        s = sn\n",
    "        t += 1\n",
    "    \n",
    "    print('episode {} finished in {} steps'.format(e, t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monte-Carlo policy gradient (bad idea since estimate of Q is always the same)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "done = False\n",
    "discount = 1.0\n",
    "render = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, env, learning_rate):\n",
    "        self.nA = env.action_space.n\n",
    "        n_components = 100\n",
    "        self.lr = learning_rate\n",
    "        self.feature_memory = []\n",
    "        observation_examples = np.array([np.concatenate(([env.action_space.sample()], \n",
    "                                                         env.observation_space.sample())) for x in range(100000)])\n",
    "\n",
    "        self.scaler = StandardScaler()\n",
    "        self.scaler.fit(observation_examples)\n",
    "\n",
    "        self.feature_map = FeatureUnion([(\"rbf1\", RBFSampler(n_components=n_components, gamma=1., random_state=1)),\n",
    "                                         (\"rbf01\", RBFSampler(n_components=n_components, gamma=0.1, random_state=1)),\n",
    "                                         (\"rbf10\", RBFSampler(n_components=n_components, gamma=10, random_state=1))])\n",
    "\n",
    "        self.feature_map.fit(self.scaler.transform(observation_examples))\n",
    "        \n",
    "        self.theta = np.random.rand(3 * n_components)\n",
    "    \n",
    "    def act(self, s):\n",
    "        Phi_s = self.feature_map.transform(self.scaler.transform(\n",
    "                np.hstack((np.arange(self.nA)[:, np.newaxis], np.repeat(s[np.newaxis, :], self.nA, axis=0)))))\n",
    "        \n",
    "        self.feature_memory.append(Phi_s)\n",
    "        probs = np.exp(np.dot(Phi_s, self.theta))\n",
    "        return np.random.choice(self.nA, p=probs/np.sum(probs))\n",
    "    \n",
    "    def update(self, memory):\n",
    "        gamma = 1 # 0.999\n",
    "        # Q_samples = np.cumsum(map(lambda x: x[2], memory))[::-1] * np.cumprod(np.repeat(gamma, len(memory)))\n",
    "        Q_samples = (map(lambda x: x[2], memory))[::-1] * np.cumprod(np.repeat(gamma, len(memory)))\n",
    "        \n",
    "        for t in range(len(memory)):\n",
    "            self.theta += self.lr * (self.feature_memory[t][memory[t][1]] - \\\n",
    "                                     np.mean(self.feature_memory[t], axis=0)) * Q_samples[t]\n",
    "        \n",
    "        self.feature_memory = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "agent = Agent(env, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 0 finished in 500 steps\n",
      "episode 1 finished in 500 steps\n",
      "episode 2 finished in 500 steps\n",
      "episode 3 finished in 500 steps\n",
      "episode 4 finished in 500 steps\n",
      "episode 5 finished in 500 steps\n",
      "episode 6 finished in 500 steps\n",
      "episode 7 finished in 500 steps\n",
      "episode 8 finished in 500 steps\n",
      "episode 9 finished in 500 steps\n",
      "episode 10 finished in 500 steps\n",
      "episode 11 finished in 500 steps\n",
      "episode 12 finished in 500 steps\n",
      "episode 13 finished in 500 steps\n",
      "episode 14 finished in 500 steps\n",
      "episode 15 finished in 500 steps\n",
      "episode 16 finished in 500 steps\n",
      "episode 17 finished in 500 steps\n",
      "episode 18 finished in 500 steps\n",
      "episode 19 finished in 500 steps\n"
     ]
    }
   ],
   "source": [
    "for e in range(20):\n",
    "    s = env.reset()\n",
    "    episode = 0\n",
    "    done = False\n",
    "    memory = []\n",
    "    \n",
    "    while not done and episode < 500:\n",
    "        if render: env.render()\n",
    "        a = agent.act(s)\n",
    "        sp, r, done, _ = env.step(a)\n",
    "        memory.append((s, a, r, sp))\n",
    "        s = sp\n",
    "        episode += 1\n",
    "    \n",
    "    print('episode {} finished in {} steps'.format(e, episode))\n",
    "    \n",
    "    agent.update(memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
